{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87f30cc",
   "metadata": {},
   "source": [
    "# Healthcare Data Entities Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b397a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing and importing relevant libraries\n",
    "import glob\n",
    "import spacy\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "from time import time\n",
    "from spacy.tokenizer import Tokenizer\n",
    "SENT_RANGE = 10 # Range of word to consider as features\n",
    "\n",
    "# load the model\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "model.tokenizer = Tokenizer(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddd999e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./spike_queries/negative/educated_academy.csv\n",
      "./spike_queries/negative/educated_at_school.csv\n",
      "./spike_queries/mixed/graduated_from_school.csv\n",
      "./spike_queries/mixed/did_not_graduated.csv\n",
      "./spike_queries/mixed/results_john _studied_uni.csv\n",
      "./spike_queries/mixed/studied_academy.csv\n",
      "./spike_queries/positive/results_john _edu_uni.csv\n",
      "./spike_queries/positive/results_john _grad_uni.csv\n",
      "./spike_queries/positive/educated_college.csv\n",
      "./spike_queries/positive/grad_college.csv\n",
      "./spike_queries/positive/grad_academy.csv\n"
     ]
    }
   ],
   "source": [
    "for file_name in glob.glob(\"./spike_queries/*/*.csv\"):\n",
    "    print(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f66e5",
   "metadata": {},
   "source": [
    "# Dataset prepration and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b89e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_RE = pd.DataFrame()\n",
    "# Load query data and extract relations\n",
    "for file_name in glob.glob(\"./spike_queries/*/*.csv\"):\n",
    "    df_raw1 = pd.read_csv(file_name)\n",
    "    if len(df_RE):\n",
    "        df_RE = pd.concat([df_RE, df_raw1], ignore_index=True)\n",
    "    else:    \n",
    "        df_RE = df_raw1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "30bf9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples:  830  Number of negative samples:  830\n"
     ]
    }
   ],
   "source": [
    "none_all = []\n",
    "medical_sample = []\n",
    "with open('golden_data.txt', 'r') as data_file:\n",
    "    for line in data_file:\n",
    "        if line.endswith('||NONE\\n'):\n",
    "            none_all.append(line.replace('||NONE\\n', ''))\n",
    "        elif line.endswith('||TREAT_FOR_DIS\\n'):\n",
    "            medical_sample.append(line.replace('||TREAT_FOR_DIS\\n', ''))\n",
    "\n",
    "none_sample = none_all[:len(medical_sample)]\n",
    "print('Number of positive samples: ', len(none_sample), ' Number of negative samples: ',len(medical_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "670a349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_len  = int(len(medical_sample) * 0.7)\n",
    "train1, test1 = medical_sample[:train_len], medical_sample[train_len:] \n",
    "train2, test2 = none_sample[:train_len], none_sample[train_len:] \n",
    "train = train1 + train2\n",
    "test = test1 + test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "e24fbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentences(org_sentences):\n",
    "    sentences_formatted = []\n",
    "    annotations = []\n",
    "    single_annot = 'O'\n",
    "    for sentence in org_sentences:\n",
    "        sentence_formatted = ''\n",
    "        annotation = ''\n",
    "        for word in sentence.split():\n",
    "            if word == '<DIS>':\n",
    "                single_annot = 'D'\n",
    "\n",
    "            elif word == '</DIS>':\n",
    "                single_annot = 'O'\n",
    "\n",
    "            elif word == '<TREAT>':\n",
    "                single_annot = 'T'\n",
    "\n",
    "            elif word == '</TREAT>':\n",
    "                single_annot = 'O'\n",
    "            else:\n",
    "                sentence_formatted += word + ' '\n",
    "                annotation += single_annot + ' '\n",
    "\n",
    "        sentences_formatted.append(sentence_formatted.strip())\n",
    "        annotations.append(annotation.strip())\n",
    "        \n",
    "    return sentences_formatted, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3879fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = format_sentences(train)\n",
    "test_sentences, test_labels = format_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "23f8a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Datasets statistics: --------------------\n",
      "\n",
      "'T' - Treatment\n",
      "'D' - Disease\n",
      "'O' - Other\n",
      "------------- Train dataset -------------\n",
      "Train annotations counts\n",
      "O                           25099\n",
      "T                            2159\n",
      "D                            1956\n",
      "dtype: int64\n",
      "NER tokens('D'/'T'): 0.141%, 'O' tokens: 0.859%\n",
      "Number of sentences: 1162\n",
      "\n",
      "------------- Test dataset -------------\n",
      "Test annotations counts\n",
      "O                          8336\n",
      "T                           777\n",
      "D                           683\n",
      "dtype: int64\n",
      "NER tokens('D'/'T'): 0.149%, 'O' tokens: 0.851%\n",
      "Number of sentences: 498\n"
     ]
    }
   ],
   "source": [
    "train_count_total = [sent_lbl for sentence_labels in train_labels for sent_lbl in sentence_labels if sent_lbl != ' ']\n",
    "train_count_o = [sent_lbl for sentence_labels in train_labels for sent_lbl in sentence_labels if sent_lbl != ' ' and sent_lbl == 'O']\n",
    "O_percent = round(len(train_count_o)/len(train_count_total),3)\n",
    "ner_percent = 1 - O_percent\n",
    "df = pd.DataFrame()\n",
    "df['Train annotations counts'] = train_count_total\n",
    "print('-------------------- Datasets statistics: --------------------\\n')\n",
    "print(\"'T' - Treatment\")\n",
    "print(\"'D' - Disease\")\n",
    "print(\"'O' - Other\")\n",
    "print('------------- Train dataset -------------')\n",
    "print(df.value_counts())\n",
    "print(\"NER tokens('D'/'T'): {}%, 'O' tokens: {}%\".format(round(ner_percent,4), O_percent))\n",
    "print(\"Number of sentences: {}\".format(len(train_sentences)))\n",
    "\n",
    "\n",
    "test_count_total = [sent_lbl for sentence_labels in test_labels for sent_lbl in sentence_labels if sent_lbl != ' ']\n",
    "test_count_o = [sent_lbl for sentence_labels in test_labels for sent_lbl in sentence_labels if sent_lbl != ' ' and sent_lbl == 'O']\n",
    "O_percent = round(len(test_count_o)/len(test_count_total),3)\n",
    "ner_percent = 1 - O_percent\n",
    "df = pd.DataFrame()\n",
    "df['Test annotations counts'] = test_count_total\n",
    "\n",
    "print('\\n------------- Test dataset -------------')\n",
    "print(df.value_counts())\n",
    "print(\"NER tokens('D'/'T'): {}%, 'O' tokens: {}%\".format(round(ner_percent,4), O_percent))\n",
    "print(\"Number of sentences: {}\".format(len(test_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d4c33",
   "metadata": {},
   "source": [
    "# Defining features for CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469d43f",
   "metadata": {},
   "source": [
    "## Select one out of two options:\n",
    "1) 10 range word feature from left and right\n",
    "\n",
    "2) Previous and next word features\n",
    "\n",
    "3) Previous word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0db93d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Range of \"SENT_RANGE\" range of features\n",
    "def getFeaturesForOneWord(cur_loc, sentence):\n",
    "    end_loc = len(sentence) - 1\n",
    "\n",
    "    # Obtaining features for words\n",
    "    features = []\n",
    "    left_range = max(0, cur_loc - SENT_RANGE)\n",
    "    right_range = min(end_loc, cur_loc + 10)\n",
    "\n",
    "    for i_loc in range(left_range, right_range):\n",
    "        word = sentence[i_loc]\n",
    "        i = i_loc - cur_loc\n",
    "        features.extend([\n",
    "        f'word{i}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{i}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{i}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{i}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{i}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{i}.isupper={word.orth_.isupper()}',                            # is the word in all uppercase\n",
    "        f'word{i}.isdigit={word.orth_.isdigit()}',                            # is the word a number\n",
    "        f'word{i}.startsWithCapital={word.orth_[0].isupper()}'])               # is the word starting with a capital letter\n",
    "        \n",
    "    if(cur_loc == 0):\n",
    "        features.append('BEG')                                                # feature to track begin of sentence \n",
    " \n",
    "    elif(cur_loc == end_loc - 1):\n",
    "        features.append('END')                                                # feature to track end of sentence\n",
    " \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "30de0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Previous, current and next word features\n",
    "def getFeaturesForOneWord(cur_loc, sentence):\n",
    "    end_loc = len(sentence) - 1\n",
    "    # Obtaining features for current word\n",
    "    word = sentence[cur_loc]\n",
    " \n",
    "    features = [\n",
    "    f'word{0}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "    f'word{0}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "    f'word{0}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "    f'word{0}.dep=' + word.dep_,                                             # dependency dependent\n",
    "    f'word{0}.head=' + word.head.orth_,                                      # dependency head\n",
    "    f'word{0}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "    f'word{0}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "    f'word{0}.startsWithCapital={word.orth_[0].isupper()}']                  # is the word starting with a capital letter\n",
    "\n",
    "        \n",
    "    if(cur_loc > 0):\n",
    "        word = sentence[cur_loc - 1]\n",
    "        features.extend([\n",
    "        f'word{-1}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{-1}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{-1}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{-1}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{-1}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{-1}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "        f'word{-1}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "        f'word{-1}.startsWithCapital={word.orth_[0].isupper()}'])                # is the word starting with a capital letter\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        features.append('BEG')                                          # feature to track begin of sentence \n",
    " \n",
    "    if(cur_loc + 1 < end_loc):\n",
    "        word = sentence[cur_loc + 1]\n",
    "        features.extend([\n",
    "        f'word{1}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{1}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{1}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{1}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{1}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{1}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "        f'word{1}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "        f'word{1}.startsWithCapital={word.orth_[0].isupper()}'])                  # is the word starting with a capital letter\n",
    "\n",
    "    else:\n",
    "        features.append('END')                                                # feature to track end of sentence\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "bae5ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Previous and current word features\n",
    "def getFeaturesForOneWord(cur_loc, sentence):\n",
    "    end_loc = len(sentence) - 1\n",
    "    # Obtaining features for current word\n",
    "    word = sentence[cur_loc]\n",
    "\n",
    "    features = [\n",
    "    f'word{0}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "    f'word{0}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "    f'word{0}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "    f'word{0}.dep=' + word.dep_,                                             # dependency dependent\n",
    "    f'word{0}.head=' + word.head.orth_,                                      # dependency head\n",
    "    f'word{0}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "    f'word{0}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "    f'word{0}.startsWithCapital={word.orth_[0].isupper()}']                  # is the word starting with a capital letter\n",
    "\n",
    "        \n",
    "    if(cur_loc > 0):\n",
    "        word = sentence[cur_loc - 1]\n",
    "        features.extend([\n",
    "        f'word{-1}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{-1}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{-1}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{-1}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{-1}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{-1}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "        f'word{-1}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "        f'word{-1}.startsWithCapital={word.orth_[0].isupper()}'])                # is the word starting with a capital letter\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        features.append('BEG')                                                # feature to track begin of sentence \n",
    "\n",
    "    if(cur_loc == end_loc):\n",
    "        features.append('END')                                                # feature to track end of sentence\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4e35d",
   "metadata": {},
   "source": [
    "# Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "7cb58042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features for a sentence.\n",
    "def getFeaturesForOneSentence(sentence):\n",
    "    sentence_parsing = model(sentence)\n",
    "    return [getFeaturesForOneWord(ii, sentence_parsing) for ii,token in enumerate(sentence_parsing)]\n",
    "\n",
    "# code to get the labels for a sentence.\n",
    "def getLabelsInListForOneSentence(labels):\n",
    "    return labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "e9736100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence: \"CONCLUSION : Methylphenidate is effective in treating children with epilepsy and ADHD and safe in children who are seizure free .\"\n",
      "\n",
      "Total features in the sentence: 21\n",
      "Example of features for the word \"rates\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['word0.lower=methylphenidate',\n",
       " 'word0.postag=NOUN',\n",
       " 'word0[-3:]=ate',\n",
       " 'word0.dep=nsubj',\n",
       " 'word0.head=is',\n",
       " 'word0.isupper=False',\n",
       " 'word0.isdigit=False',\n",
       " 'word0.startsWithCapital=True',\n",
       " 'word-1.lower=:',\n",
       " 'word-1.postag=PUNCT',\n",
       " 'word-1[-3:]=:',\n",
       " 'word-1.dep=punct',\n",
       " 'word-1.head=CONCLUSION',\n",
       " 'word-1.isupper=False',\n",
       " 'word-1.isdigit=False',\n",
       " 'word-1.startsWithCapital=False']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking feature extraction\n",
    "example_sentence = train_sentences[1]\n",
    "print(f'Example sentence: \"{example_sentence}\"\\n')\n",
    "\n",
    "features = getFeaturesForOneSentence(example_sentence)\n",
    "print('Total features in the sentence:', len(features))\n",
    "print('Example of features for the word \"rates\":')\n",
    "features[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "225258b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [getFeaturesForOneSentence(sentence) for sentence in train_sentences]\n",
    "X_test = [getFeaturesForOneSentence(sentence) for sentence in test_sentences]\n",
    "\n",
    "Y_train = [getLabelsInListForOneSentence(labels) for labels in train_labels]\n",
    "Y_test = [getLabelsInListForOneSentence(labels) for labels in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf3291",
   "metadata": {},
   "source": [
    "# build the CRF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "29381efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf = sklearn_crfsuite.CRF(max_iterations=300)\n",
    "\n",
    "try:\n",
    "    crf.fit(X_train, Y_train)\n",
    "except AttributeError:\n",
    "    pass\n",
    "    \n",
    "predictions = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8602e29",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "590a86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "2e3fa88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using oprtion (1) - \"Range of SENT_RANGE word features\":\n",
      "Weighted F1: 0.9222604008175131\n",
      "Macro F1: 0.7947574532912421\n",
      "Recall F1: 0.7347480494361633\n",
      "Precision F1: 0.8849859829189922\n",
      "Accuracy F1: 0.9280318497345855\n"
     ]
    }
   ],
   "source": [
    "print('Using oprtion (1) - \"Range of SENT_RANGE word features\":')\n",
    "print(\"Weighted F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='weighted')))\n",
    "print(\"Macro F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Recall F1: {}\".format(metrics.flat_recall_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Precision F1: {}\".format(metrics.flat_precision_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Accuracy F1: {}\".format(metrics.flat_accuracy_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "9836b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using oprtion (2) - \"Previous, current and next word features\":\n",
      "Weighted F1: 0.9138709791733991\n",
      "Macro F1: 0.7690004082496799\n",
      "Recall F1: 0.7004792772905395\n",
      "Precision F1: 0.8813663196202324\n",
      "Accuracy F1: 0.921600653327889\n"
     ]
    }
   ],
   "source": [
    "print('Using oprtion (2) - \"Previous, current and next word features\":')\n",
    "print(\"Weighted F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='weighted')))\n",
    "print(\"Macro F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Recall F1: {}\".format(metrics.flat_recall_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Precision F1: {}\".format(metrics.flat_precision_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Accuracy F1: {}\".format(metrics.flat_accuracy_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "2d4275a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using oprtion (3) - \"Previous and current word features\":\n",
      "Weighted F1: 0.9051708510330074\n",
      "Macro F1: 0.7440366657060755\n",
      "Recall F1: 0.6754440241483685\n",
      "Precision F1: 0.863239130302844\n",
      "Accuracy F1: 0.9146590445079624\n"
     ]
    }
   ],
   "source": [
    "print('Using oprtion (3) - \"Previous and current word features\":')\n",
    "print(\"Weighted F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='weighted')))\n",
    "print(\"Macro F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Recall F1: {}\".format(metrics.flat_recall_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Precision F1: {}\".format(metrics.flat_precision_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Accuracy F1: {}\".format(metrics.flat_accuracy_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e971fae0",
   "metadata": {},
   "source": [
    "# Extract all relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "4ae049a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = train_sentences + test_sentences\n",
    "all_sentences_string = train_sentences + test_sentences\n",
    "all_sentences = [i.split() for i in all_sentences]\n",
    "all_labels = train_labels + test_labels\n",
    "all_labels = [i.split() for i in all_labels]\n",
    "condition_treatment_evidence = {'condition':[], 'treatment':[], 'evidence':[]}            # Initializing an empty dictionary\n",
    "\n",
    "\n",
    "for i in range(len(all_labels)):\n",
    "\n",
    "\n",
    "        #print(test_sentences[i])\n",
    "        cnt_disease = 0           # Count of number of diseases mentioned in the sentence\n",
    "        cnt_treatment = 0         # Count of the number of treatments mentioned in the sentence\n",
    "        diseases = [\"\"]           # Initializing a blank list of diseases for current sentence.\n",
    "        treatment = [\"\"]          # Initializing a blank list of treatments for current sentence.\n",
    "        sentence_number = [\"\"]\n",
    "        evidence = [\"\"]\n",
    "        \n",
    "        length = len(all_labels[i])   # Length of current sentence.\n",
    "        for j in range(length):\n",
    "            if (all_labels[i][j] == 'D'):                                                     # Checking for label indicating disease for current word ('D')\n",
    "                diseases[cnt_disease] += (all_sentences[i][j] + \" \")            # Adding word to diseases list.\n",
    "                if j < length - 1:\n",
    "                    if (all_labels[i][j+1] != 'D'):                                           # Check for name of disease extending over multiple words. \n",
    "                        # If next word does not have label 'D', then truncate the space added at the end of the last word.\n",
    "                        diseases[cnt_disease] = diseases[cnt_disease][:-1]\n",
    "                        cnt_disease += 1\n",
    "                        diseases.append(\"\")                                               # Adding a placeholder for the next disease in the current sentence.\n",
    "                else:\n",
    "                    diseases[cnt_disease] = diseases[cnt_disease][:-1]\n",
    "                    cnt_disease += 1\n",
    "                    diseases.append(\"\")\n",
    "                                \n",
    "            if (all_labels[i][j] == 'T'):                                                     # Checking for label indicating treatment for current word ('T')\n",
    "                treatment[cnt_treatment] += (all_sentences[i][j] + \" \") # Adding word to corresponding treatment list.\n",
    "                if j < length - 1:\n",
    "                    if (all_labels[i][j+1] != 'T'):                                           # Check for name of treatment extending over multiple words. \n",
    "                        # If next word does not have label 'T', then truncate the space added at the end of the last word.\n",
    "                        treatment[cnt_treatment] = treatment[cnt_treatment][:-1]\n",
    "                        cnt_treatment += 1\n",
    "                        treatment.append(\"\")                                              # Adding a placeholder for the next treatment in the current sentence.\n",
    "                else:\n",
    "                    treatment[cnt_treatment] = treatment[cnt_treatment][:-1]\n",
    "                    cnt_treatment += 1\n",
    "                    treatment.append(\"\")\n",
    "\n",
    "        diseases.pop(-1)    # Getting rid of the last empty placeholder in diseases list\n",
    "        treatment.pop(-1)   # Getting rid of the last empty placeholder in treatments list\n",
    "        if cnt_disease and cnt_treatment:\n",
    "            for i_deases in range(cnt_disease):\n",
    "                for j in range(cnt_treatment):             \n",
    "                    condition_treatment_evidence['condition'].append(diseases[i_deases])            \n",
    "                    condition_treatment_evidence['treatment'].append(treatment[j])\n",
    "                    condition_treatment_evidence['evidence'].append(all_sentences_string[i])\n",
    "  \n",
    "# Create the pandas DataFrame\n",
    "df_gold = pd.DataFrame(condition_treatment_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd4a8b",
   "metadata": {},
   "source": [
    "# Train on all the data and predicting on a new queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "9f7c4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_new_relations(all_data_to_pred, all_predictions):\n",
    "    cte = {'condition':[], 'treatment':[], 'evidence':[]}\n",
    "\n",
    "    for sentence, preds in zip(data_to_pred1, Y1_pred):\n",
    "        single_treatment = []\n",
    "        single_condition = []\n",
    "        sentence_treatments = []\n",
    "        sentence_conditions = []\n",
    "        cnt_D = 0;cnt_T = 0;cnt_O = 0;\n",
    "        for word, pred in zip(sentence.split(), preds):\n",
    "            if pred == 'D':\n",
    "                cnt_D += 1; cnt_T = 0;\n",
    "                single_condition.append(word)\n",
    "                if cnt_T != 0:\n",
    "                    sentence_treatments.append(' '.join(single_treatment))\n",
    "                    single_treatment = []\n",
    "\n",
    "            elif pred == 'T':\n",
    "                cnt_T += 1; cnt_D = 0;\n",
    "                single_treatment.append(word)\n",
    "                if cnt_D != 0:\n",
    "                    sentence_conditions.append(' '.join(single_condition))\n",
    "                    single_condition = []                \n",
    "\n",
    "            elif pred == 'O':            \n",
    "                if cnt_T != 0:\n",
    "                    sentence_treatments.append(' '.join(single_treatment))\n",
    "                    single_treatment = []\n",
    "                if cnt_D != 0:\n",
    "                    sentence_conditions.append(' '.join(single_condition))\n",
    "                    single_condition = []  \n",
    "\n",
    "                cnt_D = 0; cnt_T = 0;\n",
    "\n",
    "        if cnt_T != 0:\n",
    "            sentence_treatments.append(' '.join(single_treatment))\n",
    "            single_treatment = []\n",
    "        if cnt_D != 0:\n",
    "            sentence_conditions.append(' '.join(single_condition))\n",
    "            single_condition = []  \n",
    "\n",
    "        if len(sentence_conditions) and len(sentence_treatments):\n",
    "            for t in sentence_treatments:\n",
    "                for c in sentence_conditions:\n",
    "                    cte['condition'].append(c)\n",
    "                    cte['treatment'].append(t)\n",
    "                    cte['evidence'].append(sentence)\n",
    "\n",
    "    return pd.DataFrame(cte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "ae87273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on all data\n",
    "all_sentences = train_sentences + test_sentences\n",
    "all_labels = train_labels + test_labels\n",
    "all_sentences_f = [getFeaturesForOneSentence(sentence) for sentence in all_sentences]\n",
    "predictions = crf.predict(all_sentences_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "41055101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File name:  spike_queries/results_healed_by.csv Number of realtions:  16\n",
      "File name:  spike_queries/results_treatment_with.csv Number of realtions:  1019\n",
      "File name:  spike_queries/results_treated_by.csv Number of realtions:  1683\n",
      "File name:  spike_queries/results_cured_with.csv Number of realtions:  731\n",
      "File name:  spike_queries/results_cured_by.csv Number of realtions:  255\n"
     ]
    }
   ],
   "source": [
    "df_RE = pd.DataFrame()\n",
    "# Load query data and extract relations\n",
    "for file_name in glob.glob(\"spike_queries/*.csv\"):\n",
    "    df_raw1 = pd.read_csv(file_name)\n",
    "    data_to_pred1 = df_raw1['sentence_text'].to_list()\n",
    "\n",
    "    X1_pred = [getFeaturesForOneSentence(sentence) for sentence in data_to_pred1]\n",
    "    Y1_pred = crf.predict(X1_pred)\n",
    "\n",
    "    relations = extract_new_relations(data_to_pred1, Y1_pred)\n",
    "    print('File name: ',file_name, 'Number of realtions: ',len(relations))\n",
    "    if len(df_RE):\n",
    "        df_RE = pd.concat([df_RE, relations], ignore_index=True)\n",
    "    else:    \n",
    "        df_RE = relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "1774799b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4759\n"
     ]
    }
   ],
   "source": [
    "all_re = pd.concat([df_RE, df_gold], ignore_index=True)\n",
    "print(len(all_re))\n",
    "all_re.to_json(r'relation2.jsonl',orient = 'records', lines = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "33cb9f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A(O) case(O) of(O) tuberculous(D) mastitis(D) not(O) cured(O) by(O) prolonged(T) streptomycin(T) therapy(T) .(T) \n",
      "A(O) case(O) of(O) tuberculous(D) mastitis(D) cured(O) by(O) prolonged(T) streptomycin(T) therapy(T) .(T) \n"
     ]
    }
   ],
   "source": [
    "# Example for Re that should not be extracted\n",
    "txt1 = 'A case of tuberculous mastitis not cured by prolonged streptomycin therapy .'\n",
    "yy1 = crf.predict([getFeaturesForOneSentence(txt1)])\n",
    "txt2 = 'A case of tuberculous mastitis cured by prolonged streptomycin therapy .'\n",
    "yy2 = crf.predict([getFeaturesForOneSentence(txt2)])\n",
    "\n",
    "strr = ''\n",
    "for i,j in zip(txt1.split(),yy1[0]):\n",
    "    strr += i+'('+j+') '\n",
    "strr2 = ''\n",
    "for i,j in zip(txt2.split(),yy2[0]):\n",
    "    strr2 += i+'('+j+') '\n",
    "    \n",
    "print(strr)\n",
    "print(strr2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
