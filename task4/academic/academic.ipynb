{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87f30cc",
   "metadata": {},
   "source": [
    "# Healthcare Data Entities Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "b397a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing and importing relevant libraries\n",
    "import glob\n",
    "import spacy\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import metrics\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import os\n",
    "from time import time\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "SENT_RANGE = 10 # Range of word to consider as features\n",
    "NAME = 'N'\n",
    "INSTITUTION = 'I'\n",
    "OTHER = 'O'\n",
    "SCHOOL_LIST = ['high', 'primary', 'intermediate', 'secondary']\n",
    "INST_LIST = [\"university\", \"college\", \"institute\", \"academy\", \"institution\"]\n",
    "\n",
    "# load the model\n",
    "model = spacy.load(\"en_core_web_sm\")\n",
    "model.tokenizer = Tokenizer(model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2331dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_loc(txt, ner_char_start, ner_end_char):\n",
    "    loc = 0\n",
    "    locations_list = []\n",
    "    for ii, word in enumerate(txt.split()):\n",
    "        word_len = len(word)\n",
    "        if ner_char_start <= loc <= ner_end_char:\n",
    "            locations_list.append(ii)\n",
    "        loc += word_len + 1\n",
    "        \n",
    "    return locations_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f66e5",
   "metadata": {},
   "source": [
    "# Dataset prepration and overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b682da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive samples:  200  Number of negative samples:  24\n"
     ]
    }
   ],
   "source": [
    "df_RE = pd.DataFrame()\n",
    "# Load query data and extract relations\n",
    "for file_name in glob.glob(\"./spike_queries/*/*.csv\"):\n",
    "    df_raw1 = pd.read_csv(file_name)\n",
    "    if len(df_RE):\n",
    "        df_RE = pd.concat([df_RE, df_raw1], ignore_index=True)\n",
    "    else:    \n",
    "        df_RE = df_raw1\n",
    "df_RE_neg = df_RE[(df_RE['label'] == 0) &(df_RE['Yale_last_index'] != '?')]\n",
    "df_RE_pos = df_RE[(df_RE['label'] == 1) &(df_RE['Yale_last_index'] != '?')]\n",
    "\n",
    "print('Number of positive samples: ', len(df_RE_pos), ' Number of negative samples: ',len(df_RE_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "670a349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_len  = int(len(df_RE_pos) * 0.7)\n",
    "train_neg_len  = int(len(df_RE_neg) * 0.7)\n",
    "\n",
    "train1, test1 = df_RE_pos[:train_pos_len], df_RE_pos[train_pos_len:] \n",
    "train2, test2 = df_RE_neg[:train_neg_len], df_RE_neg[train_neg_len:]\n",
    "train = pd.concat([train1, train2], ignore_index=True)\n",
    "test = pd.concat([test1, test2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e24fbe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_sentences(org_sentences):\n",
    "    sentences_formatted = []\n",
    "    annotations = []\n",
    "    single_annot = 'O'\n",
    "    for idx, row in org_sentences.iterrows():\n",
    "        sentence_formatted = ''\n",
    "        annotation = ''\n",
    "\n",
    "        if str(row.label) == '1':\n",
    "            for token_i, token in enumerate(row.sentence_text.split()):\n",
    "                if int(row.John_first_index) <= token_i <= int(row.John_last_index):\n",
    "                    single_anno = NAME\n",
    "                elif int(row.Yale_first_index) <= token_i <= int(row.Yale_last_index):\n",
    "                    single_anno = INSTITUTION\n",
    "                else:\n",
    "                    single_anno = OTHER\n",
    "\n",
    "                sentence_formatted += token + ' '\n",
    "                annotation += single_anno + ' '\n",
    "\n",
    "        if str(row.label) == '0':\n",
    "            for token_i, token in enumerate(row.sentence_text.split()):               \n",
    "                sentence_formatted += token + ' '\n",
    "                annotation += OTHER + ' '\n",
    "\n",
    "        sentences_formatted.append(sentence_formatted.strip())\n",
    "        annotations.append(annotation.strip())\n",
    "        \n",
    "    return sentences_formatted, annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3879fee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = format_sentences(train)\n",
    "test_sentences, test_labels = format_sentences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23f8a611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Datasets statistics: --------------------\n",
      "\n",
      "'N' - Name\n",
      "'I' - Institution\n",
      "'O' - Other\n",
      "------------- Train dataset -------------\n",
      "Train annotations counts\n",
      "O                           2546\n",
      "I                            439\n",
      "N                            186\n",
      "dtype: int64\n",
      "NER tokens('N'/'I'): 0.197%, 'O' tokens: 0.803%\n",
      "Number of sentences: 156\n",
      "\n",
      "------------- Test dataset -------------\n",
      "Test annotations counts\n",
      "O                          1195\n",
      "I                           183\n",
      "N                            89\n",
      "dtype: int64\n",
      "NER tokens('N'/'I'): 0.185%, 'O' tokens: 0.815%\n",
      "Number of sentences: 68\n"
     ]
    }
   ],
   "source": [
    "train_count_total = [sent_lbl for sentence_labels in train_labels for sent_lbl in sentence_labels if sent_lbl != ' ']\n",
    "train_count_o = [sent_lbl for sentence_labels in train_labels for sent_lbl in sentence_labels if sent_lbl != ' ' and sent_lbl == 'O']\n",
    "O_percent = round(len(train_count_o)/len(train_count_total),3)\n",
    "ner_percent = 1 - O_percent\n",
    "df = pd.DataFrame()\n",
    "df['Train annotations counts'] = train_count_total\n",
    "print('-------------------- Datasets statistics: --------------------\\n')\n",
    "print(\"'N' - Name\")\n",
    "print(\"'I' - Institution\")\n",
    "print(\"'O' - Other\")\n",
    "print('------------- Train dataset -------------')\n",
    "print(df.value_counts())\n",
    "print(\"NER tokens('N'/'I'): {}%, 'O' tokens: {}%\".format(round(ner_percent,4), O_percent))\n",
    "print(\"Number of sentences: {}\".format(len(train_sentences)))\n",
    "\n",
    "\n",
    "test_count_total = [sent_lbl for sentence_labels in test_labels for sent_lbl in sentence_labels if sent_lbl != ' ']\n",
    "test_count_o = [sent_lbl for sentence_labels in test_labels for sent_lbl in sentence_labels if sent_lbl != ' ' and sent_lbl == 'O']\n",
    "O_percent = round(len(test_count_o)/len(test_count_total),3)\n",
    "ner_percent = 1 - O_percent\n",
    "df = pd.DataFrame()\n",
    "df['Test annotations counts'] = test_count_total\n",
    "\n",
    "print('\\n------------- Test dataset -------------')\n",
    "print(df.value_counts())\n",
    "print(\"NER tokens('N'/'I'): {}%, 'O' tokens: {}%\".format(round(ner_percent,4), O_percent))\n",
    "print(\"Number of sentences: {}\".format(len(test_sentences)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d4c33",
   "metadata": {},
   "source": [
    "# Defining features for CRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2469d43f",
   "metadata": {},
   "source": [
    "## Select one out of two options:\n",
    "1) 10 range word feature from left and right\n",
    "\n",
    "2) Previous and next word features\n",
    "\n",
    "3) Previous word features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c396e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_loc(txt, ner_char_start, ner_end_char):\n",
    "    loc = 0\n",
    "    locations_list = []\n",
    "    for ii, word in enumerate(txt.split()):\n",
    "        word_len = len(word)\n",
    "        if ner_char_start <= loc <= ner_end_char:\n",
    "            locations_list.append(ii)\n",
    "        loc += word_len + 1\n",
    "        \n",
    "    return locations_list\n",
    "\n",
    "def get_entities(sentence):\n",
    "    sentence_org = ' '.join([token.orth_ for token in sentence])\n",
    "    person_locs = []\n",
    "    org_locs = []\n",
    "\n",
    "    if sentence.ents:\n",
    "        for ent in sentence.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                person_locs.extend(get_token_loc(sentence_org, ent.start_char, ent.end_char))\n",
    "            if ent.label_ == 'ORG':\n",
    "                org_locs.extend(get_token_loc(sentence_org, ent.start_char, ent.end_char))\n",
    "                \n",
    "    return person_locs, org_locs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0db93d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Range of \"SENT_RANGE\" range of features\n",
    "def getFeaturesForOneWord(cur_loc, sentence):\n",
    "    person_locs, org_locs = get_entities(sentence)\n",
    "    end_loc = len(sentence) - 1\n",
    "\n",
    "    # Obtaining features for words\n",
    "    features = []\n",
    "    left_range = max(0, cur_loc - SENT_RANGE)\n",
    "    right_range = min(end_loc, cur_loc + 10)\n",
    "\n",
    "    for i_loc in range(left_range, right_range):\n",
    "        word = sentence[i_loc]\n",
    "        i = i_loc - cur_loc\n",
    "        features.extend([\n",
    "        f'word{i}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{i}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{i}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{i}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{i}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{i}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "        f'word{i}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "        f'word{i}.person_ent={i_loc in person_locs}',                            # is this word part of person NER\n",
    "        f'word{i}.inst_ent={i_loc in org_locs}',                                 # is this word part of institution NER\n",
    "        f'word{i}.school={any([sub_w in word.orth_.lower() for sub_w in SCHOOL_LIST])}', # school ind\n",
    "        f'word{i}.ins={any([sub_w in word.orth_.lower() for sub_w in INST_LIST])}',      # institution ind            \n",
    "        f'word{i}.startsWithCapital={word.orth_[0].isupper()}'])                 # is the word starting with a capital letter\n",
    "        \n",
    "    if(cur_loc == 0):\n",
    "        features.append('BEG')                                                   # feature to track begin of sentence \n",
    " \n",
    "    elif(cur_loc == end_loc - 1):\n",
    "        features.append('END')                                                   # feature to track end of sentence\n",
    " \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "30de0952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Previous, current and next word features\n",
    "def getFeaturesForOneWord(cur_loc, sentence):\n",
    "    person_locs, org_locs = get_entities(sentence)    \n",
    "    end_loc = len(sentence) - 1\n",
    "    # Obtaining features for current word\n",
    "    word = sentence[cur_loc]\n",
    " \n",
    "    features = [\n",
    "    f'word{0}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "    f'word{0}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "    f'word{0}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "    f'word{0}.dep=' + word.dep_,                                             # dependency dependent\n",
    "    f'word{0}.head=' + word.head.orth_,                                      # dependency head\n",
    "    f'word{0}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "    f'word{0}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "    f'word{0}.startsWithCapital={word.orth_[0].isupper()}']                  # is the word starting with a capital letter\n",
    "    f'word{0}.person_ent={cur_loc in person_locs}',                            # is this word part of person NER\n",
    "    f'word{0}.inst_ent={cur_loc in org_locs}',                                 # is this word part of institution NER\n",
    "    f'word{0}.school={any([sub_w in word.orth_.lower() for sub_w in SCHOOL_LIST])}', # school ind\n",
    "    f'word{0}.ins={any([sub_w in word.orth_.lower() for sub_w in INST_LIST])}',      # institution ind      \n",
    "        \n",
    "    if(cur_loc > 0):\n",
    "        prev_loc = cur_loc - 1\n",
    "        word = sentence[prev_loc]\n",
    "        features.extend([\n",
    "        f'word{-1}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{-1}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{-1}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{-1}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{-1}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{-1}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "        f'word{-1}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "        f'word{-1}.startsWithCapital={word.orth_[0].isupper()}'])                # is the word starting with a capital letter\n",
    "        f'word{-1}.person_ent={prev_loc in person_locs}',                            # is this word part of person NER\n",
    "        f'word{-1}.inst_ent={prev_loc in org_locs}',                                 # is this word part of institution NER\n",
    "        f'word{-1}.school={any([sub_w in word.orth_.lower() for sub_w in SCHOOL_LIST])}', # school ind\n",
    "        f'word{-1}.ins={any([sub_w in word.orth_.lower() for sub_w in INST_LIST])}',      # institution ind  \n",
    "        \n",
    "    else:\n",
    "        features.append('BEG')                                          # feature to track begin of sentence \n",
    " \n",
    "    if(cur_loc + 1 < end_loc):\n",
    "        next_loc = cur_loc + 1\n",
    "        word = sentence[next_loc]\n",
    "        features.extend([\n",
    "        f'word{1}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{1}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{1}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{1}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{1}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{1}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "        f'word{1}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "        f'word{1}.startsWithCapital={word.orth_[0].isupper()}'])                  # is the word starting with a capital letter\n",
    "        f'word{1}.person_ent={next_loc in person_locs}',                            # is this word part of person NER\n",
    "        f'word{1}.inst_ent={next_loc in org_locs}',                                 # is this word part of institution NER\n",
    "        f'word{1}.school={any([sub_w in word.orth_.lower() for sub_w in SCHOOL_LIST])}', # school ind\n",
    "        f'word{1}.ins={any([sub_w in word.orth_.lower() for sub_w in INST_LIST])}',      # institution ind     \n",
    "    else:\n",
    "        features.append('END')                                                # feature to track end of sentence\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "bae5ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Previous and current word features\n",
    "def getFeaturesForOneWord(cur_loc, sentence):\n",
    "    end_loc = len(sentence) - 1\n",
    "    # Obtaining features for current word\n",
    "    word = sentence[cur_loc]\n",
    "\n",
    "    features = [\n",
    "    f'word{0}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "    f'word{0}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "    f'word{0}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "    f'word{0}.dep=' + word.dep_,                                             # dependency dependent\n",
    "    f'word{0}.head=' + word.head.orth_,                                      # dependency head\n",
    "    f'word{0}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "    f'word{0}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "    f'word{0}.startsWithCapital={word.orth_[0].isupper()}']                  # is the word starting with a capital letter\n",
    "    f'word{0}.person_ent={cur_loc in person_locs}',                            # is this word part of person NER\n",
    "    f'word{0}.inst_ent={cur_loc in org_locs}',                                 # is this word part of institution NER\n",
    "    f'word{0}.school={any([sub_w in word.orth_.lower() for sub_w in SCHOOL_LIST])}', # school ind\n",
    "    f'word{0}.ins={any([sub_w in word.orth_.lower() for sub_w in INST_LIST])}',      # institution ind   \n",
    "        \n",
    "    if(cur_loc > 0):\n",
    "        prev_loc = cur_loc - 1\n",
    "        word = sentence[prev_loc]\n",
    "        features.extend([\n",
    "        f'word{-1}.lower=' + word.orth_.lower(),                                  # serves as word id\n",
    "        f'word{-1}.postag=' + word.pos_,                                          # PoS tag of current word\n",
    "        f'word{-1}[-3:]=' + word.orth_[-3:],                                      # last three characters\n",
    "        f'word{-1}.dep=' + word.dep_,                                             # dependency dependent\n",
    "        f'word{-1}.head=' + word.head.orth_,                                      # dependency head\n",
    "        f'word{-1}.isupper={word.orth_.isupper()}',                               # is the word in all uppercase\n",
    "        f'word{-1}.isdigit={word.orth_.isdigit()}',                               # is the word a number\n",
    "        f'word{-1}.startsWithCapital={word.orth_[0].isupper()}'])                # is the word starting with a capital letter\n",
    "        f'word{-1}.person_ent={prev_loc in person_locs}',                            # is this word part of person NER\n",
    "        f'word{-1}.inst_ent={prev_loc in org_locs}',                                 # is this word part of institution NER\n",
    "        f'word{-1}.school={any([sub_w in word.orth_.lower() for sub_w in SCHOOL_LIST])}', # school ind\n",
    "        f'word{-1}.ins={any([sub_w in word.orth_.lower() for sub_w in INST_LIST])}',      # institution ind          \n",
    "    \n",
    "    else:\n",
    "        features.append('BEG')                                                # feature to track begin of sentence \n",
    "\n",
    "    if(cur_loc == end_loc):\n",
    "        features.append('END')                                                # feature to track end of sentence\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c4e35d",
   "metadata": {},
   "source": [
    "# Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7cb58042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features for a sentence.\n",
    "def getFeaturesForOneSentence(sentence):\n",
    "    sentence_parsing = model(sentence)\n",
    "    return [getFeaturesForOneWord(ii, sentence_parsing) for ii,token in enumerate(sentence_parsing)]\n",
    "\n",
    "# code to get the labels for a sentence.\n",
    "def getLabelsInListForOneSentence(labels):\n",
    "    return labels.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e9736100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(New Haven, 'GPE'), (Connecticut, 'GPE'), (Mitchell, 'PERSON'), (Cheshire Academy, 'ORG'), (1863, 'DATE')]\n",
      "Example sentence: \"Born in New Haven , Connecticut , Mitchell was graduated from Cheshire Academy in 1863 .\"\n",
      "\n",
      "Total features in the sentence: 16\n",
      "Example of features for the word \"rates\":\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['word-10.lower=new',\n",
       " 'word-10.postag=PROPN',\n",
       " 'word-10[-3:]=New',\n",
       " 'word-10.dep=compound',\n",
       " 'word-10.head=Haven',\n",
       " 'word-10.isupper=False',\n",
       " 'word-10.isdigit=False',\n",
       " 'word-10.person_ent=False',\n",
       " 'word-10.inst_ent=False',\n",
       " 'word-10.school=False',\n",
       " 'word-10.ins=False',\n",
       " 'word-10.startsWithCapital=True',\n",
       " 'word-9.lower=haven',\n",
       " 'word-9.postag=PROPN',\n",
       " 'word-9[-3:]=ven',\n",
       " 'word-9.dep=pobj',\n",
       " 'word-9.head=in',\n",
       " 'word-9.isupper=False',\n",
       " 'word-9.isdigit=False',\n",
       " 'word-9.person_ent=False',\n",
       " 'word-9.inst_ent=False',\n",
       " 'word-9.school=False',\n",
       " 'word-9.ins=False',\n",
       " 'word-9.startsWithCapital=True',\n",
       " 'word-8.lower=,',\n",
       " 'word-8.postag=PUNCT',\n",
       " 'word-8[-3:]=,',\n",
       " 'word-8.dep=punct',\n",
       " 'word-8.head=Haven',\n",
       " 'word-8.isupper=False',\n",
       " 'word-8.isdigit=False',\n",
       " 'word-8.person_ent=False',\n",
       " 'word-8.inst_ent=False',\n",
       " 'word-8.school=False',\n",
       " 'word-8.ins=False',\n",
       " 'word-8.startsWithCapital=False',\n",
       " 'word-7.lower=connecticut',\n",
       " 'word-7.postag=PROPN',\n",
       " 'word-7[-3:]=cut',\n",
       " 'word-7.dep=conj',\n",
       " 'word-7.head=Haven',\n",
       " 'word-7.isupper=False',\n",
       " 'word-7.isdigit=False',\n",
       " 'word-7.person_ent=False',\n",
       " 'word-7.inst_ent=False',\n",
       " 'word-7.school=False',\n",
       " 'word-7.ins=False',\n",
       " 'word-7.startsWithCapital=True',\n",
       " 'word-6.lower=,',\n",
       " 'word-6.postag=PUNCT',\n",
       " 'word-6[-3:]=,',\n",
       " 'word-6.dep=punct',\n",
       " 'word-6.head=graduated',\n",
       " 'word-6.isupper=False',\n",
       " 'word-6.isdigit=False',\n",
       " 'word-6.person_ent=False',\n",
       " 'word-6.inst_ent=False',\n",
       " 'word-6.school=False',\n",
       " 'word-6.ins=False',\n",
       " 'word-6.startsWithCapital=False',\n",
       " 'word-5.lower=mitchell',\n",
       " 'word-5.postag=PROPN',\n",
       " 'word-5[-3:]=ell',\n",
       " 'word-5.dep=nsubjpass',\n",
       " 'word-5.head=graduated',\n",
       " 'word-5.isupper=False',\n",
       " 'word-5.isdigit=False',\n",
       " 'word-5.person_ent=True',\n",
       " 'word-5.inst_ent=False',\n",
       " 'word-5.school=False',\n",
       " 'word-5.ins=False',\n",
       " 'word-5.startsWithCapital=True',\n",
       " 'word-4.lower=was',\n",
       " 'word-4.postag=AUX',\n",
       " 'word-4[-3:]=was',\n",
       " 'word-4.dep=auxpass',\n",
       " 'word-4.head=graduated',\n",
       " 'word-4.isupper=False',\n",
       " 'word-4.isdigit=False',\n",
       " 'word-4.person_ent=False',\n",
       " 'word-4.inst_ent=False',\n",
       " 'word-4.school=False',\n",
       " 'word-4.ins=False',\n",
       " 'word-4.startsWithCapital=False',\n",
       " 'word-3.lower=graduated',\n",
       " 'word-3.postag=VERB',\n",
       " 'word-3[-3:]=ted',\n",
       " 'word-3.dep=ROOT',\n",
       " 'word-3.head=graduated',\n",
       " 'word-3.isupper=False',\n",
       " 'word-3.isdigit=False',\n",
       " 'word-3.person_ent=False',\n",
       " 'word-3.inst_ent=False',\n",
       " 'word-3.school=False',\n",
       " 'word-3.ins=False',\n",
       " 'word-3.startsWithCapital=False',\n",
       " 'word-2.lower=from',\n",
       " 'word-2.postag=ADP',\n",
       " 'word-2[-3:]=rom',\n",
       " 'word-2.dep=prep',\n",
       " 'word-2.head=graduated',\n",
       " 'word-2.isupper=False',\n",
       " 'word-2.isdigit=False',\n",
       " 'word-2.person_ent=False',\n",
       " 'word-2.inst_ent=False',\n",
       " 'word-2.school=False',\n",
       " 'word-2.ins=False',\n",
       " 'word-2.startsWithCapital=False',\n",
       " 'word-1.lower=cheshire',\n",
       " 'word-1.postag=PROPN',\n",
       " 'word-1[-3:]=ire',\n",
       " 'word-1.dep=compound',\n",
       " 'word-1.head=Academy',\n",
       " 'word-1.isupper=False',\n",
       " 'word-1.isdigit=False',\n",
       " 'word-1.person_ent=False',\n",
       " 'word-1.inst_ent=True',\n",
       " 'word-1.school=False',\n",
       " 'word-1.ins=False',\n",
       " 'word-1.startsWithCapital=True',\n",
       " 'word0.lower=academy',\n",
       " 'word0.postag=PROPN',\n",
       " 'word0[-3:]=emy',\n",
       " 'word0.dep=pobj',\n",
       " 'word0.head=from',\n",
       " 'word0.isupper=False',\n",
       " 'word0.isdigit=False',\n",
       " 'word0.person_ent=False',\n",
       " 'word0.inst_ent=True',\n",
       " 'word0.school=False',\n",
       " 'word0.ins=True',\n",
       " 'word0.startsWithCapital=True',\n",
       " 'word1.lower=in',\n",
       " 'word1.postag=ADP',\n",
       " 'word1[-3:]=in',\n",
       " 'word1.dep=prep',\n",
       " 'word1.head=graduated',\n",
       " 'word1.isupper=False',\n",
       " 'word1.isdigit=False',\n",
       " 'word1.person_ent=False',\n",
       " 'word1.inst_ent=False',\n",
       " 'word1.school=False',\n",
       " 'word1.ins=False',\n",
       " 'word1.startsWithCapital=False',\n",
       " 'word2.lower=1863',\n",
       " 'word2.postag=NUM',\n",
       " 'word2[-3:]=863',\n",
       " 'word2.dep=pobj',\n",
       " 'word2.head=in',\n",
       " 'word2.isupper=False',\n",
       " 'word2.isdigit=True',\n",
       " 'word2.person_ent=False',\n",
       " 'word2.inst_ent=False',\n",
       " 'word2.school=False',\n",
       " 'word2.ins=False',\n",
       " 'word2.startsWithCapital=False']"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking feature extraction\n",
    "example_sentence = train_sentences[1]\n",
    "doc = model(example_sentence)\n",
    "print([(i, i.label_) for i in doc.ents])\n",
    "print(f'Example sentence: \"{example_sentence}\"\\n')\n",
    "\n",
    "features = getFeaturesForOneSentence(example_sentence)\n",
    "print('Total features in the sentence:', len(features))\n",
    "print('Example of features for the word \"rates\":')\n",
    "features[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "225258b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [getFeaturesForOneSentence(sentence) for sentence in train_sentences]\n",
    "X_test = [getFeaturesForOneSentence(sentence) for sentence in test_sentences]\n",
    "\n",
    "Y_train = [getLabelsInListForOneSentence(labels) for labels in train_labels]\n",
    "Y_test = [getLabelsInListForOneSentence(labels) for labels in test_labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbf3291",
   "metadata": {},
   "source": [
    "# build the CRF Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "29381efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crf(X_train, Y_train):\n",
    "    crf = sklearn_crfsuite.CRF(max_iterations=300)\n",
    "\n",
    "    try:\n",
    "        crf.fit(X_train, Y_train)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    return crf\n",
    "\n",
    "crf = get_crf(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8602e29",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "590a86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = crf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "2e3fa88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using oprtion (1) - \"Range of SENT_RANGE word features\":\n",
      "Weighted F1: 0.9653213629660603\n",
      "Macro F1: 0.9248861689432877\n",
      "Recall F1: 0.9229805085367687\n",
      "Precision F1: 0.9303775276880901\n",
      "Accuracy F1: 0.9652351738241309\n"
     ]
    }
   ],
   "source": [
    "print('Using oprtion (1) - \"Range of SENT_RANGE word features\":')\n",
    "print(\"Weighted F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='weighted')))\n",
    "print(\"Macro F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Recall F1: {}\".format(metrics.flat_recall_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Precision F1: {}\".format(metrics.flat_precision_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Accuracy F1: {}\".format(metrics.flat_accuracy_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9836b470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using oprtion (2) - \"Previous, current and next word features\":\n",
      "Weighted F1: 0.9555766763664667\n",
      "Macro F1: 0.9144131815126414\n",
      "Recall F1: 0.9180163967822992\n",
      "Precision F1: 0.9143847169808743\n",
      "Accuracy F1: 0.9550102249488752\n"
     ]
    }
   ],
   "source": [
    "print('Using oprtion (2) - \"Previous, current and next word features\":')\n",
    "print(\"Weighted F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='weighted')))\n",
    "print(\"Macro F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Recall F1: {}\".format(metrics.flat_recall_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Precision F1: {}\".format(metrics.flat_precision_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Accuracy F1: {}\".format(metrics.flat_accuracy_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "2d4275a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using oprtion (3) - \"Previous and current word features\":\n",
      "Weighted F1: 0.9440871294983808\n",
      "Macro F1: 0.8927344357876477\n",
      "Recall F1: 0.9053001773710566\n",
      "Precision F1: 0.8896958738860005\n",
      "Accuracy F1: 0.9427402862985685\n"
     ]
    }
   ],
   "source": [
    "print('Using oprtion (3) - \"Previous and current word features\":')\n",
    "print(\"Weighted F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='weighted')))\n",
    "print(\"Macro F1: {}\".format(metrics.flat_f1_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Recall F1: {}\".format(metrics.flat_recall_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Precision F1: {}\".format(metrics.flat_precision_score(Y_test, Y_pred, average='macro')))\n",
    "print(\"Accuracy F1: {}\".format(metrics.flat_accuracy_score(Y_test, Y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482fc97",
   "metadata": {},
   "source": [
    "# Extract all relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6a8359c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = train_sentences + test_sentences\n",
    "all_sentences_string = train_sentences + test_sentences\n",
    "all_sentences = [i.split() for i in all_sentences]\n",
    "all_labels = train_labels + test_labels\n",
    "all_labels = [i.split() for i in all_labels]\n",
    "condition_treatment_evidence = {'person':[], 'institution':[], 'evidence':[]}\n",
    "\n",
    "\n",
    "for i in range(len(all_labels)):\n",
    "\n",
    "\n",
    "        #print(test_sentences[i])\n",
    "        cnt_disease = 0           # Count of number of diseases mentioned in the sentence\n",
    "        cnt_treatment = 0         # Count of the number of treatments mentioned in the sentence\n",
    "        diseases = [\"\"]           # Initializing a blank list of diseases for current sentence.\n",
    "        treatment = [\"\"]          # Initializing a blank list of treatments for current sentence.\n",
    "        sentence_number = [\"\"]\n",
    "        evidence = [\"\"]\n",
    "        \n",
    "        length = len(all_labels[i])   # Length of current sentence.\n",
    "        for j in range(length):\n",
    "            if (all_labels[i][j] == 'N'):                                                     # Checking for label indicating disease for current word ('D')\n",
    "                diseases[cnt_disease] += (all_sentences[i][j] + \" \")            # Adding word to diseases list.\n",
    "                if j < length - 1:\n",
    "                    if (all_labels[i][j+1] != 'N'):                                           # Check for name of disease extending over multiple words. \n",
    "                        # If next word does not have label 'D', then truncate the space added at the end of the last word.\n",
    "                        diseases[cnt_disease] = diseases[cnt_disease][:-1]\n",
    "                        cnt_disease += 1\n",
    "                        diseases.append(\"\")                                               # Adding a placeholder for the next disease in the current sentence.\n",
    "                else:\n",
    "                    diseases[cnt_disease] = diseases[cnt_disease][:-1]\n",
    "                    cnt_disease += 1\n",
    "                    diseases.append(\"\")\n",
    "                                \n",
    "            if (all_labels[i][j] == 'I'):                                                     # Checking for label indicating treatment for current word ('T')\n",
    "                treatment[cnt_treatment] += (all_sentences[i][j] + \" \") # Adding word to corresponding treatment list.\n",
    "                if j < length - 1:\n",
    "                    if (all_labels[i][j+1] != 'I'):                                           # Check for name of treatment extending over multiple words. \n",
    "                        # If next word does not have label 'T', then truncate the space added at the end of the last word.\n",
    "                        treatment[cnt_treatment] = treatment[cnt_treatment][:-1]\n",
    "                        cnt_treatment += 1\n",
    "                        treatment.append(\"\")                                              # Adding a placeholder for the next treatment in the current sentence.\n",
    "                else:\n",
    "                    treatment[cnt_treatment] = treatment[cnt_treatment][:-1]\n",
    "                    cnt_treatment += 1\n",
    "                    treatment.append(\"\")\n",
    "\n",
    "        diseases.pop(-1)    # Getting rid of the last empty placeholder in diseases list\n",
    "        treatment.pop(-1)   # Getting rid of the last empty placeholder in treatments list\n",
    "        if cnt_disease and cnt_treatment:\n",
    "            for i_deases in range(cnt_disease):\n",
    "                for j in range(cnt_treatment):             \n",
    "                    condition_treatment_evidence['person'].append(diseases[i_deases])            \n",
    "                    condition_treatment_evidence['institution'].append(treatment[j])\n",
    "                    condition_treatment_evidence['evidence'].append(all_sentences_string[i])\n",
    "  \n",
    "# Create the pandas DataFrame\n",
    "df_gold = pd.DataFrame(condition_treatment_evidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd4a8b",
   "metadata": {},
   "source": [
    "# Train on all the data and predicting on a new queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9f7c4bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_new_relations(all_data_to_pred, all_predictions):\n",
    "    cte = {'person':[], 'institution':[], 'evidence':[]}\n",
    "\n",
    "    for sentence, preds in zip(data_to_pred1, Y1_pred):\n",
    "        single_treatment = []\n",
    "        single_condition = []\n",
    "        sentence_treatments = []\n",
    "        sentence_conditions = []\n",
    "        cnt_N = 0;cnt_I = 0;cnt_O = 0;\n",
    "        for word, pred in zip(sentence.split(), preds):\n",
    "            if pred == NAME:\n",
    "                cnt_N += 1; cnt_I = 0;\n",
    "                single_condition.append(word)\n",
    "                if cnt_I != 0:\n",
    "                    sentence_treatments.append(' '.join(single_treatment))\n",
    "                    single_treatment = []\n",
    "\n",
    "            elif pred == INSTITUTION:\n",
    "                cnt_I += 1; cnt_N = 0;\n",
    "                single_treatment.append(word)\n",
    "                if cnt_N != 0:\n",
    "                    sentence_conditions.append(' '.join(single_condition))\n",
    "                    single_condition = []                \n",
    "\n",
    "            elif pred == 'O':            \n",
    "                if cnt_I != 0:\n",
    "                    sentence_treatments.append(' '.join(single_treatment))\n",
    "                    single_treatment = []\n",
    "                if cnt_N != 0:\n",
    "                    sentence_conditions.append(' '.join(single_condition))\n",
    "                    single_condition = []  \n",
    "\n",
    "                cnt_N = 0; cnt_I = 0;\n",
    "\n",
    "        if cnt_I != 0:\n",
    "            sentence_treatments.append(' '.join(single_treatment))\n",
    "            single_treatment = []\n",
    "        if cnt_N != 0:\n",
    "            sentence_conditions.append(' '.join(single_condition))\n",
    "            single_condition = []  \n",
    "\n",
    "        if len(sentence_conditions) and len(sentence_treatments):\n",
    "            for t in sentence_treatments:\n",
    "                for c in sentence_conditions:\n",
    "                    cte['person'].append(c)\n",
    "                    cte['institution'].append(t)\n",
    "                    cte['evidence'].append(sentence)\n",
    "\n",
    "    return pd.DataFrame(cte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ae87273d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model on all data\n",
    "all_sentences = train_sentences + test_sentences\n",
    "all_labels = train_labels + test_labels\n",
    "all_sentences_f = [getFeaturesForOneSentence(sentence) for sentence in all_sentences]\n",
    "all_labels_joined = [getLabelsInListForOneSentence(labels) for labels in all_labels]\n",
    "\n",
    "crf = get_crf(all_sentences_f, all_labels_joined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "8ed1d50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spike_to_pred/bs_studied_in.csv 45065\n",
      "spike_to_pred/bs_graduated_from.csv 45924\n",
      "spike_to_pred/bs_graduated_at.csv 14140\n"
     ]
    }
   ],
   "source": [
    "df_all_raw = pd.DataFrame()\n",
    "# Load query data and extract relations\n",
    "for file_name in glob.glob(\"spike_to_pred/*.csv\"):\n",
    "    df_raw1 = pd.read_csv(file_name)\n",
    "    print(file_name, len(df_raw1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "41a1ed34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THe total length 105353\n",
      "THe total unique length 94022\n"
     ]
    }
   ],
   "source": [
    "#get unique values\n",
    "df_all_raw = pd.DataFrame()\n",
    "# Load query data and extract relations\n",
    "for file_name in glob.glob(\"spike_to_pred/*.csv\"):\n",
    "    df_raw1 = pd.read_csv(file_name)\n",
    "    df_all_raw = pd.concat([df_all_raw, df_raw1], ignore_index=True)\n",
    "    \n",
    "df_all_raw = pd.concat([df_all_raw, df_RE_neg[['sentence_id','sentence_text']]], ignore_index=True)\n",
    "df_all_raw = pd.concat([df_all_raw, df_RE_pos[['sentence_id','sentence_text']]], ignore_index=True)\n",
    "print(\"THe total length\",len(df_all_raw))\n",
    "\n",
    "df_uniq = pd.DataFrame()\n",
    "df_uniq['sentence_text'] = df_all_raw.sentence_text.unique()\n",
    "\n",
    "print(\"THe total unique length\",len(df_uniq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "41055101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of extracted realtions:  29114\n",
      "Number of all realtions:  29314\n"
     ]
    }
   ],
   "source": [
    "data_to_pred1 = df_uniq['sentence_text'].to_list()\n",
    "\n",
    "X1_pred = [getFeaturesForOneSentence(sentence) for sentence in data_to_pred1]\n",
    "Y1_pred = crf.predict(X1_pred)\n",
    "\n",
    "df_relations = extract_new_relations(data_to_pred1, Y1_pred)\n",
    "print('Number of extracted realtions: ',len(df_relations))\n",
    "df_all_re = pd.concat([df_relations, df_gold], ignore_index=True)\n",
    "print('Number of all realtions: ',len(df_all_re))\n",
    "df_all_re.to_json(r'relation1.jsonl',orient = 'records', lines = 'True')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
