{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1faff3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am so <mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e703e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/or/.virtualenvs/biu_task2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "I : 100\n",
      " am : 524\n",
      " so : 98\n",
      "<mask> : 50264\n",
      "</s> : 2\n",
      "\n",
      "------------------------\n",
      "\n",
      "5 most similar to 'am':\n",
      ">>> I am so  am,    (probability:0.9998922348022461)\n",
      ">>> I am so  is,    (probability:3.9378628571284935e-05)\n",
      ">>> I am so 'm,    (probability:2.9937518775113858e-05)\n",
      ">>> I am so  was,    (probability:8.688964953762479e-06)\n",
      ">>> I am so  feel,    (probability:8.550764505343977e-06)\n",
      "\n",
      "------------------------\n",
      "5 most similar to '<mask>':\n",
      ">>> I am so  sorry,    (probability:0.3083705008029938)\n",
      ">>> I am so  proud,    (probability:0.0649036392569542)\n",
      ">>> I am so  grateful,    (probability:0.05806168541312218)\n",
      ">>> I am so  happy,    (probability:0.04478686675429344)\n",
      ">>> I am so  blessed,    (probability:0.032352522015571594)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaModel\n",
    "\n",
    "model_checkpoint = 'roberta-base'\n",
    "RobertaLM_model = RobertaForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "for code in inputs['input_ids'][0]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model_output = RobertaLM_model(**inputs)\n",
    "    logits = model_output.logits\n",
    "    \n",
    "am_loc = 2\n",
    "mask_loc = 4\n",
    "\n",
    "def k_most_similar(logits, index):\n",
    "    mask_token_logits = logits[0, index, :]\n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    probabilities = F.softmax(mask_token_logits,dim=0)\n",
    "    top_5_tokens = np.argsort(-probabilities)[:5].tolist()\n",
    "    \n",
    "    for token in top_5_tokens:\n",
    "        print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))},    (probability:{probabilities[token]})\")\n",
    "\n",
    "print('\\n------------------------\\n')\n",
    "print(\"5 most similar to 'am':\") \n",
    "k_most_similar(logits, am_loc)\n",
    "print('\\n------------------------')\n",
    "print(\"5 most similar to '<mask>':\") \n",
    "k_most_similar(logits, mask_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a0ce70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Static word embeddings\n",
    "#all_embeddings = RobertaLM_model.roberta.embeddings.word_embeddings.weight\n",
    "#am_embeddings = all_embeddings[tokenizer([\"I am so <mask>\"])['input_ids'][0][am_loc]]\n",
    "#mask_embeddings = all_embeddings[tokenizer([\"I am so <mask>\"])['input_ids'][0][mask_loc]]\n",
    "\n",
    "# Contextualize word embeddings\n",
    "Roberta_model = RobertaModel.from_pretrained(model_checkpoint)\n",
    "with torch.no_grad():\n",
    "    model_output = Roberta_model(**inputs)['last_hidden_state']\n",
    "    \n",
    "am_embeddings = model_output[0][am_loc]\n",
    "mask_embeddings = model_output[0][mask_loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1d586cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9720)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Low\n",
    "sentence1 = \"The experience was dope, I enjoyed it so much\"\n",
    "sentence2 = \"The drugs addict bought an dope\"\n",
    "\n",
    "# High\n",
    "sentence1 = \"I took a loan from the bank\"\n",
    "sentence2 = \"I went to the bank to get money\"\n",
    "\n",
    "input1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "input2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "Roberta_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output1 = Roberta_model(**input1)['last_hidden_state'][0]\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model_output2 = Roberta_model(**input2)['last_hidden_state'][0]\n",
    "    \n",
    "embs1 = model_output1[7][:]\n",
    "embs2 = model_output2[5][:]\n",
    "\n",
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embs1, embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e7b124cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "I : 100\n",
      " just : 95\n",
      " love : 657\n",
      " Dil : 14205\n",
      "oph : 6673\n",
      "osaurus : 44422\n",
      "</s> : 2\n"
     ]
    }
   ],
   "source": [
    "# 4) Find a sentence with n words, that is tokenized into m > n tokens by the tokenizer.\n",
    "for code in tokenizer(\"I just love Dilophosaurus\")['input_ids']:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
