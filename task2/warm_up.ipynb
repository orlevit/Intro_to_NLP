{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1faff3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am so <mask>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70e703e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/or/.virtualenvs/biu_task2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "I : 100\n",
      " am : 524\n",
      " so : 98\n",
      "<mask> : 50264\n",
      "</s> : 2\n",
      "\n",
      "------------------------\n",
      "\n",
      "5 most similar to 'am':\n",
      ">>> I am so  am,    (probability:0.9998922348022461)\n",
      ">>> I am so  is,    (probability:3.9378628571284935e-05)\n",
      ">>> I am so 'm,    (probability:2.9937518775113858e-05)\n",
      ">>> I am so  was,    (probability:8.688964953762479e-06)\n",
      ">>> I am so  feel,    (probability:8.550764505343977e-06)\n",
      "\n",
      "------------------------\n",
      "5 most similar to '<mask>':\n",
      ">>> I am so  sorry,    (probability:0.3083705008029938)\n",
      ">>> I am so  proud,    (probability:0.0649036392569542)\n",
      ">>> I am so  grateful,    (probability:0.05806168541312218)\n",
      ">>> I am so  happy,    (probability:0.04478686675429344)\n",
      ">>> I am so  blessed,    (probability:0.032352522015571594)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, RobertaModel\n",
    "\n",
    "model_checkpoint = 'roberta-base'\n",
    "RobertaLM_model = RobertaForMaskedLM.from_pretrained(model_checkpoint)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "for code in inputs['input_ids'][0]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model_output = RobertaLM_model(**inputs)\n",
    "    logits = model_output.logits\n",
    "    \n",
    "am_loc = 2\n",
    "mask_loc = 4\n",
    "\n",
    "def k_most_similar(logits, index):\n",
    "    mask_token_logits = logits[0, index, :]\n",
    "    # Pick the [MASK] candidates with the highest logits\n",
    "    probabilities = F.softmax(mask_token_logits,dim=0)\n",
    "    top_5_tokens = np.argsort(-probabilities)[:5].tolist()\n",
    "    \n",
    "    for token in top_5_tokens:\n",
    "        print(f\">>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))},    (probability:{probabilities[token]})\")\n",
    "\n",
    "print('\\n------------------------\\n')\n",
    "print(\"5 most similar to 'am':\") \n",
    "k_most_similar(logits, am_loc)\n",
    "print('\\n------------------------')\n",
    "print(\"5 most similar to '<mask>':\") \n",
    "k_most_similar(logits, mask_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a0ce70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Static word embeddings\n",
    "#all_embeddings = RobertaLM_model.roberta.embeddings.word_embeddings.weight\n",
    "#am_embeddings = all_embeddings[tokenizer([\"I am so <mask>\"])['input_ids'][0][am_loc]]\n",
    "#mask_embeddings = all_embeddings[tokenizer([\"I am so <mask>\"])['input_ids'][0][mask_loc]]\n",
    "\n",
    "# Contextualize word embeddings\n",
    "Roberta_model = RobertaModel.from_pretrained(model_checkpoint)\n",
    "with torch.no_grad():\n",
    "    model_output = Roberta_model(**inputs)['last_hidden_state']\n",
    "    \n",
    "am_embeddings = model_output[0][am_loc]\n",
    "mask_embeddings = model_output[0][mask_loc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7b7894",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69afbe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "33b1ad09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████| 333/333 [00:00<00:00, 111kB/s]\n",
      "Downloading: 100%|████████████████████████████| 798k/798k [00:03<00:00, 210kB/s]\n",
      "Downloading: 100%|████████████████████████████| 456k/456k [00:02<00:00, 155kB/s]\n",
      "Downloading: 100%|██████████████████████████| 1.36M/1.36M [00:05<00:00, 244kB/s]\n",
      "Downloading: 100%|█████████████████████████████| 239/239 [00:00<00:00, 58.2kB/s]\n",
      "Downloading: 100%|██████████████████████████████| 653/653 [00:00<00:00, 213kB/s]\n",
      "Downloading: 100%|███████████████████████████| 329M/329M [01:32<00:00, 3.55MB/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21685/3347481261.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Tokenize sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mencoded_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msentence1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The robber was stealing money from the bank vault\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentences' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-distilroberta-v1')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-distilroberta-v1')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "sentence1 = \"The robber was stealing money from the bank vault\"\n",
    "sentence2 = \"I was fishing on the Mississippi river bank.\"\n",
    "\n",
    "input1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "input2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output1 = Roberta_model(**input1)#['last_hidden_state'][0]\n",
    "    \n",
    "# with torch.no_grad():\n",
    "#     model = Roberta_model(**input2)['last_hidden_state'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16263962",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "for i,code in enumerate(inputs['input_ids'][1]):\n",
    "    print(f\"{i} --> { tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc7b13e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367daa4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed9a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0e67734e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = 'roberta-base'\n",
    "Roberta_model = RobertaModel.from_pretrained(model_checkpoint,output_hidden_states = True)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "00a8e1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 11, 768])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Roberta_model(**input1)['last_hidden_state'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "9093ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "Roberta_model = RobertaModel.from_pretrained('roberta-base')\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = Roberta_model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1d586cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9016)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentence1 = \"I took a loan from the bank\"\n",
    "# sentence2 = \"went to the bank to get money\"\n",
    "\n",
    "sentence1 = \"The robber was stealing money from the bank vault\"\n",
    "sentence2 = \"I was fishing on the Mississippi river bank.\"\n",
    "\n",
    "input1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "input2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "\n",
    "Roberta_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output1 = Roberta_model(**input1)['last_hidden_state'][0]\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model_output2 = Roberta_model(**input2)['last_hidden_state'][0]\n",
    "    \n",
    "bank1_embs = model_output1[8]\n",
    "bank2_embs = model_output2[8]\n",
    "\n",
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(bank1_embs, bank2_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "9c2224c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.901610255241394"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " 1 - cosine(bank1_embs, bank2_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9be475de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> <s> : 0\n",
      "1 --> I : 100\n",
      "2 -->  was : 21\n",
      "3 -->  fishing : 5651\n",
      "4 -->  on : 15\n",
      "5 -->  the : 5\n",
      "6 -->  Mississippi : 5750\n",
      "7 -->  river : 4908\n",
      "8 -->  bank : 827\n",
      "9 --> . : 4\n",
      "10 --> </s> : 2\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "for i,code in enumerate(inputs['input_ids'][0]):\n",
    "    print(f\"{i} --> { tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6a832e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0192,  0.0770, -0.0123, -0.0947,  0.0821])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output1[0][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b932bc90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   133, 29364,    21,  9460,   418,    31,     5,   827, 19362,\n",
       "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d157c67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  100,   21, 5651,   15,    5, 5750, 4908,  827,    4,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ccb6465b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 --> <s> : 0\n",
      "1 --> I : 100\n",
      "2 -->  was : 21\n",
      "3 -->  fishing : 5651\n",
      "4 -->  on : 15\n",
      "5 -->  the : 5\n",
      "6 -->  Mississippi : 5750\n",
      "7 -->  river : 4908\n",
      "8 -->  bank : 827\n",
      "9 --> . : 4\n",
      "10 --> </s> : 2\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "for i,code in enumerate(inputs['input_ids'][1]):\n",
    "    print(f\"{i} --> { tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "df87d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.6570)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentence1 = \"we need to book an order for monday\" \n",
    "#sentence2 = \"I read an article in the book and it was bad, oax, dog , cat\"\n",
    "\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer_bert = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\",output_hidden_states = True)\n",
    "model.eval()\n",
    "\n",
    "sentence1 = \"click on the mouse to open windowin the computer\"\n",
    "sentence2 = \"the mouse is an animal that love cheese\"\n",
    "sentence1 = \"I took a loan from the bank\"\n",
    "sentence2 = \"went to the bank to get money\"\n",
    "sentence1 = \"horrible\"\n",
    "sentence2 = \"fine\"\n",
    "input1 = tokenizer_bert(sentence1, return_tensors=\"pt\")\n",
    "input2 = tokenizer_bert(sentence2, return_tensors=\"pt\")\n",
    "input1['output_hidden_states'] = True\n",
    "with torch.no_grad():\n",
    "    model_output1 = model(**input1)['hidden_states'][-1]#['last_hidden_state']\n",
    "\n",
    "input2['output_hidden_states'] = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output2 = model(**input2)['hidden_states'][-1]#['last_hidden_state']\n",
    "    \n",
    "#embs1 = model_output1[0][7]#[4]\n",
    "#embs2 = model_output2[0][4]#[2]\n",
    "embs1 = model_output1[0][1]#[4]\n",
    "embs2 = model_output2[0][1]#[2]\n",
    "\n",
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embs1, embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "cee7c5b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_len=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bab9b37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2044, 11065,  2769,  2013,  1996,  2924, 11632,  1010,  1996,\n",
       "          2924, 27307,  2001,  2464,  5645,  2006,  1996,  5900,  2314,  2924,\n",
       "          1012,   102]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "e7730ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   0,  100,   21, 5651,   15,    5, 5750, 4908,  827,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f573aa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"The robber was stealing money from the bank vault\"\n",
    "text2 =        \"I was fishing on the Mississippi river bank\"\n",
    "inputs = tokenizer_bert(text1, return_tensors=\"pt\")\n",
    "# for code in inputs['input_ids'][0]:\n",
    "#     print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs)\n",
    "inputs = tokenizer_bert(text2, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs2 = model(**inputs)\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(outputs1['hidden_states'][-1][0][8], outputs2['hidden_states'][-1][0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ca7e5b98",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31634584069252014"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "18a0f5f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3163)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(outputs1['hidden_states'][-1][0][8],outputs2['hidden_states'][-1][0][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c1f87c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [ C L S ] : 101\n",
      "1: i : 1045\n",
      "2: w a s : 2001\n",
      "3: f i s h i n g : 5645\n",
      "4: o n : 2006\n",
      "5: t h e : 1996\n",
      "6: m i s s i s s i p p i : 5900\n",
      "7: r i v e r : 2314\n",
      "8: b a n k : 2924\n",
      "9: [ S E P ] : 102\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer_bert(text2, return_tensors=\"pt\")\n",
    "for i,code in enumerate(inputs['input_ids'][0]):\n",
    "    print(f\"{i}: { tokenizer_bert.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e90e9ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6978819370269775"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_bank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae6cd2ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 22, 768])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs['hidden_states'][-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb6bab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f2a794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.1404,  0.4852, -0.2654,  ..., -0.2033,  0.0936,  0.0939],\n",
       "         [ 0.4549,  0.4488, -0.3614,  ...,  0.2909,  0.2794, -0.5092],\n",
       "         [ 0.8003,  0.1645, -0.3999,  ...,  0.1224, -0.7517, -0.2463]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[-8.2017e-01, -1.7302e-01,  3.6870e-01,  6.5230e-01, -2.6350e-01,\n",
       "         -5.0332e-02,  8.8083e-01,  2.0498e-01, -2.1210e-02, -9.9967e-01,\n",
       "          1.1388e-01,  3.0782e-01,  9.5688e-01, -1.2867e-01,  8.9957e-01,\n",
       "         -5.3771e-01, -1.7128e-01, -5.0038e-01,  3.2841e-01, -7.6873e-01,\n",
       "          4.8446e-01,  8.6860e-01,  4.9746e-01,  1.8733e-01,  3.6306e-01,\n",
       "          3.8558e-01, -5.0272e-01,  8.8007e-01,  9.2353e-01,  6.0863e-01,\n",
       "         -6.4101e-01,  1.3125e-01, -9.6348e-01, -1.6425e-01,  3.6321e-01,\n",
       "         -9.6277e-01,  1.0158e-01, -6.9240e-01,  3.2903e-02,  2.7733e-02,\n",
       "         -8.0736e-01,  2.0906e-01,  9.9478e-01, -1.4673e-01, -4.3981e-02,\n",
       "         -3.0228e-01, -9.9964e-01,  2.0815e-01, -7.9279e-01, -2.2526e-01,\n",
       "         -8.6025e-02, -4.2895e-01,  1.5188e-01,  3.6276e-01,  2.9804e-01,\n",
       "          1.2521e-01, -9.2589e-02,  1.1781e-01, -1.5831e-02, -4.8618e-01,\n",
       "         -5.1622e-01,  2.3568e-01,  3.8220e-02, -8.5127e-01, -9.1503e-02,\n",
       "         -4.6129e-01, -5.3405e-02, -1.4546e-01,  7.6934e-03, -8.2204e-02,\n",
       "          7.9767e-01,  1.2084e-01,  3.5941e-01, -6.6825e-01, -2.0160e-01,\n",
       "          1.4252e-01, -4.5344e-01,  1.0000e+00, -3.6189e-01, -9.4755e-01,\n",
       "         -3.9209e-01, -9.4390e-02,  3.3248e-01,  4.6627e-01, -4.2564e-01,\n",
       "         -9.9997e-01,  2.1980e-01, -9.8851e-02, -9.7302e-01,  2.2644e-01,\n",
       "          2.4106e-01, -1.4978e-01, -5.9225e-01,  3.6714e-01, -1.7485e-01,\n",
       "         -1.2346e-01, -1.9334e-01,  3.6351e-01, -9.1667e-02, -5.7943e-03,\n",
       "         -5.0384e-02, -1.5009e-01,  2.0236e-02, -1.7679e-01,  1.0683e-01,\n",
       "         -3.0763e-01, -4.1147e-01,  1.1501e-01, -1.4143e-01,  5.6443e-01,\n",
       "          2.5642e-01, -2.3228e-01,  2.5221e-01, -9.1680e-01,  5.5899e-01,\n",
       "         -2.0600e-01, -9.5936e-01, -4.2787e-01, -9.6997e-01,  5.1934e-01,\n",
       "         -3.1424e-02, -1.3025e-01,  9.2813e-01,  5.1209e-01,  2.1117e-01,\n",
       "          1.3707e-02,  2.1689e-01, -1.0000e+00, -4.0651e-01, -5.3496e-02,\n",
       "          1.2627e-01,  1.3542e-02, -9.4535e-01, -8.9582e-01,  4.7407e-01,\n",
       "          9.1522e-01,  3.6287e-02,  9.9145e-01, -2.2511e-01,  8.5836e-01,\n",
       "          6.3043e-02, -1.2289e-01, -1.9271e-01, -3.4992e-01,  1.7838e-01,\n",
       "          2.7425e-01, -6.0479e-01,  1.7010e-01,  1.1081e-01, -3.5024e-02,\n",
       "          1.1732e-02, -2.3253e-01,  1.5170e-01, -8.8710e-01, -3.6096e-01,\n",
       "          9.0380e-01,  2.6130e-01,  2.3354e-01,  5.2452e-01, -1.7658e-01,\n",
       "         -3.2341e-01,  7.2366e-01,  2.8221e-01,  2.7295e-01, -3.8280e-02,\n",
       "          3.6192e-01, -1.1718e-01,  3.3227e-01, -7.4650e-01,  1.3715e-01,\n",
       "          3.1469e-01, -1.6775e-01,  4.6643e-01, -9.4703e-01, -2.1863e-01,\n",
       "          4.1035e-01,  9.6638e-01,  6.3347e-01,  1.0703e-01,  1.4605e-01,\n",
       "         -1.5503e-01,  2.7710e-01, -8.6703e-01,  9.4128e-01, -1.2421e-01,\n",
       "          1.5294e-01,  5.4881e-01, -1.4433e-01, -7.8261e-01, -4.0244e-01,\n",
       "          7.8887e-01, -1.8666e-01, -7.6416e-01,  7.7369e-02, -3.4658e-01,\n",
       "         -3.0308e-01,  2.7101e-01,  3.8271e-01, -1.9122e-01, -3.0257e-01,\n",
       "          3.3471e-02,  8.6690e-01,  9.5314e-01,  7.4972e-01, -5.3219e-01,\n",
       "          5.0504e-01, -8.4207e-01, -2.5497e-01,  9.4747e-02,  1.7633e-01,\n",
       "          1.1351e-01,  9.8427e-01,  1.2497e-01, -1.2000e-01, -8.8653e-01,\n",
       "         -9.6692e-01, -1.6563e-02, -8.6142e-01,  4.4836e-02, -5.5288e-01,\n",
       "          2.3028e-01,  6.8109e-01, -2.6243e-02,  3.2494e-01, -9.6605e-01,\n",
       "         -7.0974e-01,  3.3566e-01, -1.6262e-01,  2.8137e-01, -1.9041e-01,\n",
       "         -6.7370e-03, -1.5886e-01, -4.0589e-01,  7.8256e-01,  7.6211e-01,\n",
       "          4.8942e-01, -6.0985e-01,  8.1633e-01, -2.4050e-01,  8.0009e-01,\n",
       "         -4.9657e-01,  9.4614e-01, -5.4396e-02,  4.3882e-01, -8.7431e-01,\n",
       "          3.8244e-01, -8.6108e-01,  1.4426e-01, -4.3886e-02, -5.9402e-01,\n",
       "         -1.0001e-01,  3.9861e-01,  2.4723e-01,  7.6874e-01, -4.1875e-01,\n",
       "          9.9188e-01,  4.0840e-02, -8.9844e-01,  4.4698e-01, -1.2342e-01,\n",
       "         -9.5907e-01, -1.0293e-01,  2.0651e-01, -6.1455e-01, -2.5765e-01,\n",
       "         -3.7574e-01, -8.9476e-01,  8.7665e-01,  1.1135e-01,  9.7532e-01,\n",
       "          1.3311e-01, -8.8922e-01, -2.9445e-01, -7.7514e-01, -2.0106e-01,\n",
       "         -3.6219e-02,  5.5734e-01, -1.6189e-01, -9.3522e-01,  3.4827e-01,\n",
       "          3.9508e-01,  3.2141e-01,  4.8894e-01,  9.8985e-01,  9.9648e-01,\n",
       "          9.4500e-01,  8.3225e-01,  8.6101e-01, -7.2988e-01, -2.4915e-02,\n",
       "          9.9985e-01, -4.5093e-01, -9.9987e-01, -9.0514e-01, -5.1989e-01,\n",
       "          2.4296e-01, -1.0000e+00, -5.8073e-02,  5.2558e-02, -8.6958e-01,\n",
       "         -2.5128e-01,  9.4316e-01,  9.8071e-01, -9.9999e-01,  8.2534e-01,\n",
       "          9.0099e-01, -4.9088e-01,  2.1993e-01, -1.3896e-01,  9.4246e-01,\n",
       "          2.2507e-01,  3.4359e-01, -1.2264e-01,  2.3629e-01,  3.1420e-02,\n",
       "         -7.8285e-01,  3.4823e-01,  3.1673e-01,  2.8163e-01,  1.0838e-01,\n",
       "         -6.1180e-01, -8.7154e-01, -1.1642e-01, -4.6426e-02, -1.9092e-01,\n",
       "         -9.1504e-01, -4.1759e-02, -2.0472e-01,  5.9684e-01,  2.3006e-02,\n",
       "          1.1114e-01, -7.3755e-01,  1.4000e-01, -6.7656e-01,  3.3039e-01,\n",
       "          5.2308e-01, -8.8996e-01, -5.3788e-01, -1.6255e-01, -3.3300e-01,\n",
       "          2.9949e-01, -8.9050e-01,  9.4657e-01, -2.7746e-01,  6.4076e-02,\n",
       "          9.9999e-01, -1.3565e-01, -7.9296e-01,  2.9825e-01,  9.2485e-02,\n",
       "          7.2276e-02,  9.9998e-01,  3.3298e-01, -9.3960e-01, -4.0654e-01,\n",
       "          4.7582e-02, -3.0236e-01, -2.9836e-01,  9.9483e-01, -1.0621e-01,\n",
       "          2.5670e-01,  4.2536e-01,  9.2401e-01, -9.6460e-01, -1.4922e-02,\n",
       "         -8.6040e-01, -9.2954e-01,  9.2732e-01,  8.6656e-01, -1.8544e-01,\n",
       "         -5.4816e-01,  5.0818e-02,  3.7692e-02,  1.6536e-01, -9.3569e-01,\n",
       "          6.0011e-01,  4.1177e-01, -5.0632e-02,  8.4497e-01, -8.3127e-01,\n",
       "         -3.8217e-01,  3.4556e-01, -6.2195e-03,  2.3298e-01, -2.8302e-01,\n",
       "          4.1466e-01, -1.8687e-01,  1.0061e-01, -2.3792e-01,  8.2264e-02,\n",
       "         -9.4954e-01, -9.2192e-02,  9.9998e-01,  7.8572e-02, -4.0141e-01,\n",
       "         -2.0698e-01,  1.2149e-02, -3.6881e-01,  2.4953e-01,  3.0982e-01,\n",
       "         -2.5437e-01, -7.3265e-01, -1.5382e-01, -9.1197e-01, -9.5740e-01,\n",
       "          6.6954e-01,  1.2440e-01, -1.9138e-01,  9.9749e-01,  2.1803e-01,\n",
       "          8.6925e-02, -8.6045e-02,  1.9109e-01, -1.1428e-02,  5.4260e-01,\n",
       "         -3.6750e-01,  9.3390e-01, -1.4144e-01,  3.9145e-01,  7.5854e-01,\n",
       "          1.0345e-01, -2.1722e-01, -5.6786e-01, -2.2976e-02, -8.5057e-01,\n",
       "          2.5003e-02, -8.9219e-01,  9.1124e-01, -2.9871e-01,  2.5383e-01,\n",
       "          5.4446e-02, -2.4813e-01,  9.9999e-01,  1.7857e-01,  4.8846e-01,\n",
       "         -6.0896e-01,  8.4601e-01, -5.7546e-01, -6.5316e-01, -2.9532e-01,\n",
       "          2.1612e-02,  3.4582e-01, -1.6968e-01,  1.3428e-01, -9.3845e-01,\n",
       "         -2.4363e-01, -1.9950e-01, -9.6606e-01, -9.7915e-01,  4.9485e-01,\n",
       "          7.1005e-01,  5.0842e-02, -5.3849e-03, -5.9601e-01, -5.0822e-01,\n",
       "          2.7104e-01, -1.5217e-01, -8.8883e-01,  4.5539e-01, -1.9880e-01,\n",
       "          3.7463e-01, -1.5444e-01,  3.7314e-01, -3.6181e-01,  7.2956e-01,\n",
       "          6.3206e-01,  3.8260e-02,  2.6481e-02, -7.3576e-01,  7.4389e-01,\n",
       "         -7.1899e-01,  2.3589e-01, -1.1393e-01,  1.0000e+00, -3.6442e-01,\n",
       "         -1.9002e-01,  6.3109e-01,  5.7196e-01,  2.4435e-02,  1.4349e-01,\n",
       "         -3.6869e-01,  2.9116e-02,  3.3919e-01,  3.6498e-01, -8.2738e-01,\n",
       "         -2.0841e-01,  4.7572e-01, -5.2553e-01, -4.3123e-01,  6.7604e-01,\n",
       "          3.5748e-02,  4.3059e-02,  5.5922e-02, -5.6323e-04,  9.9747e-01,\n",
       "         -2.1887e-01, -1.0935e-01, -4.1619e-01,  2.7476e-02, -2.3241e-01,\n",
       "         -6.0210e-01,  9.9991e-01,  3.4109e-01, -1.0855e-01, -9.7216e-01,\n",
       "          2.7484e-01, -8.7626e-01,  9.9103e-01,  6.4935e-01, -8.1727e-01,\n",
       "          4.4894e-01,  3.4648e-01, -8.2355e-02,  7.2991e-01, -1.3814e-01,\n",
       "         -1.7177e-01,  8.3305e-02,  8.6502e-02,  9.1716e-01, -3.0777e-01,\n",
       "         -9.1214e-01, -4.6731e-01,  3.1160e-01, -9.3115e-01,  7.4362e-01,\n",
       "         -4.3319e-01, -7.1007e-02, -2.0247e-01,  4.0980e-01,  7.8675e-01,\n",
       "         -1.0492e-01, -9.4740e-01, -1.2549e-01, -1.8998e-02,  9.2377e-01,\n",
       "          9.5817e-02, -3.7589e-01, -8.3828e-01, -3.9925e-01, -4.9281e-02,\n",
       "          3.4023e-01, -8.9198e-01,  9.2673e-01, -9.6243e-01,  3.4905e-01,\n",
       "          9.9990e-01,  2.2469e-01, -6.2758e-01,  1.5701e-01, -2.9831e-01,\n",
       "          1.8165e-01,  2.3577e-01,  4.8827e-01, -9.0596e-01, -1.6562e-01,\n",
       "         -4.0251e-02,  2.1791e-01, -1.2783e-01,  2.9610e-01,  5.0220e-01,\n",
       "          6.2159e-02, -3.7343e-01, -4.1037e-01, -9.0929e-02,  3.3973e-01,\n",
       "          7.1517e-01, -2.9982e-01, -1.3527e-01,  1.1614e-01, -4.8758e-02,\n",
       "         -8.7502e-01, -1.1465e-01, -1.0866e-01, -9.3776e-01,  5.3327e-01,\n",
       "         -1.0000e+00, -3.0043e-01, -4.4450e-01, -1.6435e-01,  7.7634e-01,\n",
       "         -4.6502e-03,  5.0924e-03, -6.3994e-01,  2.3920e-01,  7.5352e-01,\n",
       "          6.6888e-01, -8.3001e-02,  3.2281e-03, -5.9993e-01,  1.3473e-01,\n",
       "         -7.9412e-02,  1.2492e-01,  9.9058e-02,  6.5058e-01, -1.2375e-01,\n",
       "          1.0000e+00,  4.2984e-02, -4.7765e-01, -9.5100e-01,  1.8728e-01,\n",
       "         -1.9012e-01,  9.9845e-01, -8.6424e-01, -8.7923e-01,  1.2846e-01,\n",
       "         -2.7867e-01, -6.9860e-01,  1.1355e-01, -6.5988e-02, -5.2468e-01,\n",
       "          1.8980e-01,  9.2924e-01,  8.8057e-01, -3.7097e-01,  2.7984e-01,\n",
       "         -2.5859e-01, -4.1873e-01,  5.3930e-03, -4.4376e-01,  9.6124e-01,\n",
       "          8.1967e-02,  8.0977e-01,  5.3288e-01,  1.4154e-01,  9.1825e-01,\n",
       "          8.6598e-02,  6.3107e-01,  9.3397e-02,  9.9993e-01,  2.6845e-01,\n",
       "         -8.5654e-01,  3.3287e-01, -9.6778e-01, -8.5634e-02, -9.1154e-01,\n",
       "          1.4378e-01,  5.6829e-02,  7.9359e-01, -2.0156e-01,  9.2116e-01,\n",
       "          4.3437e-01,  5.4958e-02,  1.7515e-02,  5.3289e-01,  2.7874e-01,\n",
       "         -8.5583e-01, -9.6289e-01, -9.6842e-01,  2.5187e-01, -4.1312e-01,\n",
       "         -3.4429e-02,  2.2130e-01,  1.2683e-01,  2.3691e-01,  3.1630e-01,\n",
       "         -9.9987e-01,  8.5874e-01,  3.0211e-01, -2.6789e-01,  9.1967e-01,\n",
       "          2.5410e-01,  2.3988e-01,  1.6861e-01, -9.6698e-01, -9.3943e-01,\n",
       "         -2.3403e-01, -2.3228e-01,  6.7307e-01,  5.0725e-01,  7.2419e-01,\n",
       "          3.0762e-01, -3.9811e-01, -6.8009e-02,  3.5426e-01, -1.3333e-01,\n",
       "         -9.7622e-01,  3.0109e-01,  1.1998e-01, -9.4596e-01,  9.0574e-01,\n",
       "         -4.7384e-01, -1.4432e-01,  5.7687e-01,  1.6875e-01,  8.9834e-01,\n",
       "          6.9709e-01,  3.9960e-01,  1.0054e-01,  3.1816e-01,  8.2568e-01,\n",
       "          9.2822e-01,  9.6815e-01,  1.9047e-01,  7.3098e-01,  1.9472e-01,\n",
       "          2.7086e-01,  3.3167e-01, -8.7915e-01,  8.2337e-02, -8.6675e-02,\n",
       "         -7.7512e-02,  1.2354e-01, -1.5033e-01, -9.3905e-01,  5.2095e-01,\n",
       "         -9.0497e-02,  3.4446e-01, -3.3507e-01,  1.9612e-01, -3.3195e-01,\n",
       "         -1.5781e-01, -6.1090e-01, -2.9556e-01,  4.9922e-01,  2.3879e-01,\n",
       "          8.4643e-01,  2.6615e-02,  8.3669e-03, -5.8020e-01, -7.9492e-02,\n",
       "          3.0855e-01, -8.3470e-01,  8.8937e-01,  3.0234e-02,  3.7097e-01,\n",
       "         -3.0413e-01, -1.4438e-01,  4.7504e-01, -3.7664e-01, -2.8270e-01,\n",
       "         -2.3053e-01, -5.7026e-01,  8.1580e-01, -5.5922e-02, -3.3569e-01,\n",
       "         -3.3704e-01,  5.7875e-01,  2.4804e-01,  9.0908e-01,  2.2234e-01,\n",
       "          4.7882e-02, -2.8296e-02, -1.1028e-01,  2.0457e-01, -1.4817e-01,\n",
       "         -9.9988e-01,  3.5730e-01,  1.8480e-01, -2.5334e-01,  2.4507e-02,\n",
       "         -4.0218e-01, -8.2222e-02, -9.5466e-01, -2.4247e-02, -2.4689e-01,\n",
       "         -2.9738e-01, -4.2388e-01, -3.6564e-01,  3.7952e-01,  2.8857e-01,\n",
       "         -2.7178e-02,  7.7198e-01,  2.9533e-01,  5.4170e-01,  4.6899e-01,\n",
       "          4.1441e-01, -5.7948e-01,  8.5186e-01]], grad_fn=<TanhBackward0>), hidden_states=(tensor([[[ 0.1686, -0.2858, -0.3261,  ..., -0.0276,  0.0383,  0.1640],\n",
       "         [ 0.3329, -0.3256, -0.5911,  ...,  1.0688, -0.9336, -1.1046],\n",
       "         [-0.4815, -0.0189,  0.0092,  ..., -0.2806,  0.3895, -0.2815]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.8220e-01, -2.5572e-02, -9.2748e-02,  ..., -6.9054e-02,\n",
       "           7.1696e-02, -8.9124e-04],\n",
       "         [ 9.1144e-01,  2.1363e-01, -1.6432e-01,  ...,  1.2779e+00,\n",
       "          -6.8556e-01, -1.5678e+00],\n",
       "         [-2.3910e-01,  3.1683e-01, -1.5588e-02,  ..., -2.8465e-01,\n",
       "           8.7918e-01, -1.0674e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0391, -0.1942, -0.1872,  ...,  0.0246,  0.0275,  0.1130],\n",
       "         [ 0.7525,  0.4619,  0.1677,  ...,  1.4907, -0.5572, -1.7443],\n",
       "         [-0.2849,  0.1194,  0.1148,  ..., -0.1296,  0.6637, -0.0269]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0369, -0.2969, -0.0220,  ...,  0.2020,  0.0742,  0.3412],\n",
       "         [ 0.8026,  0.2797,  0.3302,  ...,  1.3169, -0.7982, -1.8134],\n",
       "         [-0.0788, -0.0838,  0.1156,  ...,  0.0248,  0.1266,  0.0096]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.1370, -0.4547, -0.5191,  ...,  0.3270, -0.0823,  0.7891],\n",
       "         [ 0.9040,  0.8519, -0.2025,  ...,  0.8296, -0.9179, -1.6784],\n",
       "         [-0.0394, -0.0382,  0.0170,  ...,  0.0042,  0.0670, -0.0222]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1853, -0.3629, -0.4523,  ..., -0.4231,  0.0543,  0.5695],\n",
       "         [ 0.4598,  1.0240,  0.1818,  ...,  0.2374, -1.0404, -1.3353],\n",
       "         [-0.0230, -0.0316,  0.0169,  ...,  0.0256,  0.0059, -0.0371]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.5857e-01, -6.6535e-02, -4.6253e-01,  ..., -5.2245e-01,\n",
       "           6.7948e-02,  6.1342e-01],\n",
       "         [ 3.3669e-01,  1.2336e+00, -3.7679e-02,  ...,  2.0064e-01,\n",
       "          -8.2773e-01, -9.4694e-01],\n",
       "         [ 4.8736e-04, -2.2696e-02, -1.8619e-02,  ...,  1.6469e-02,\n",
       "          -2.1397e-02, -3.5805e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.8489,  0.0026, -0.9653,  ..., -0.4000,  0.0529,  0.6973],\n",
       "         [-0.0165,  0.9672, -1.0542,  ...,  0.0465, -0.7228, -0.6180],\n",
       "         [-0.0261, -0.0423, -0.0285,  ..., -0.0115,  0.0105, -0.0529]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3802,  0.4176, -1.3173,  ..., -1.0484, -0.0436,  0.6406],\n",
       "         [-0.4383,  0.8538, -0.7751,  ..., -0.3398, -0.9047, -0.9003],\n",
       "         [-0.0055, -0.0203,  0.0071,  ..., -0.0464, -0.0443, -0.0699]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.7375,  0.6516, -1.4555,  ..., -0.5710, -0.1383,  0.5390],\n",
       "         [-0.3524,  0.8358, -0.6400,  ..., -0.1551, -0.3774, -0.6912],\n",
       "         [-0.0045,  0.0142, -0.0165,  ..., -0.0826, -0.0918,  0.0023]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.9006,  0.6381, -1.1855,  ..., -0.3611, -0.5125,  0.4107],\n",
       "         [-0.0201,  0.6260, -0.7614,  ..., -0.0343, -0.1722, -0.6935],\n",
       "         [ 0.0145,  0.0363, -0.0685,  ...,  0.0642, -0.0535, -0.0041]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5604,  0.7330, -0.9056,  ..., -0.6276, -0.1873,  0.2460],\n",
       "         [ 0.0440,  0.7109, -0.6229,  ..., -0.2356, -0.1012, -0.6855],\n",
       "         [ 0.0297,  0.0147, -0.0105,  ...,  0.0154, -0.0532,  0.0120]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.1404,  0.4852, -0.2654,  ..., -0.2033,  0.0936,  0.0939],\n",
       "         [ 0.4549,  0.4488, -0.3614,  ...,  0.2909,  0.2794, -0.5092],\n",
       "         [ 0.8003,  0.1645, -0.3999,  ...,  0.1224, -0.7517, -0.2463]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**input1).j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d97643",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input1['output_hidden_states'] = True\n",
    "with torch.no_grad():\n",
    "    model_output2 = model(**input1)\n",
    "model_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1972322",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860afaf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c61a0fa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20658/3735043497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#embs1 = model_output1[0][7]#[4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#embs2 = model_output2[0][4]#[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0membs1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#[4]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0membs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#[2]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "sentence2 = \"went to the bank to get money\"\n",
    "sentence1 = \"good\"\n",
    "sentence2 = \"fine\"\n",
    "input1 = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "input2 = tokenizer(sentence2, return_tensors=\"pt\")\n",
    "input1['output_hidden_states'] = True\n",
    "with torch.no_grad():\n",
    "    model_output1 = model(**input1)#['hidden_states'][-1]#['last_hidden_state']\n",
    "\n",
    "input2['output_hidden_states'] = True\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output2 = model(**input2)#['hidden_states'][-1]#['last_hidden_state']\n",
    "    \n",
    "#embs1 = model_output1[0][7]#[4]\n",
    "#embs2 = model_output2[0][4]#[2]\n",
    "embs1 = model_output1[0][1]#[4]\n",
    "embs2 = model_output2[0][1]#[2]\n",
    "\n",
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embs1, embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "82e11ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7396)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "embs1 = model_output1['hidden_states'][-2][0][1]#[4]\n",
    "embs2 = model_output2['hidden_states'][-2][0][1]#[2]\n",
    "\n",
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embs1, embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c509ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb5bc435",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20658/673801503.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{ tokenizer.decode(code)} : {code}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "for code in inputs['input_ids'][1]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd268222",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a02dd04",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20658/1218119927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "174d688a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5099)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embs1, embs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9534de1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5099])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity = torch.nn.CosineSimilarity()\n",
    "cos_similarity(torch.unsqueeze(embs1,0), torch.unsqueeze(embs2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df209c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 768])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "c3b1550b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 768])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7637712e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ C L S ] : 101\n",
      "i : 1045\n",
      "t o o k : 2165\n",
      "a : 1037\n",
      "l o a n : 5414\n",
      "f r o m : 2013\n",
      "t h e : 1996\n",
      "b a n k : 2924\n",
      "[ S E P ] : 102\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "for code in inputs['input_ids'][1]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "411adc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "I : 100\n",
      " sit : 2662\n",
      " on : 15\n",
      " the : 5\n",
      " river : 4908\n",
      " bank : 827\n",
      "</s> : 2\n"
     ]
    }
   ],
   "source": [
    "inputs['output_hidden_states'] = True\n",
    "for code in inputs['input_ids'][0]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8144f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence1, sentence2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "2c7ff2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "went : 31135\n",
      " to : 7\n",
      " the : 5\n",
      " bank : 827\n",
      " to : 7\n",
      " get : 120\n",
      " money : 418\n",
      "</s> : 2\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"went to the bank to get money\", return_tensors=\"pt\")\n",
    "inputs['output_hidden_states'] = True\n",
    "for code in inputs['input_ids'][0]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "e8bb0c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 768])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1 = torch.mean(all_embeddings[tokenizer([\"good\"])['input_ids'][0][1:-1]], dim=0)\n",
    "embeddings2 = torch.mean(all_embeddings[[tokenizer([\"bad\"])['input_ids'][0][1:-1]]], dim=0)\n",
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "cf28f965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3.0015e-01, -1.1161e-01,  5.1395e-02, -6.7252e-02,  8.4492e-01,\n",
       "         2.4305e-01,  6.9192e-02, -2.4385e-02, -3.3705e-01, -3.3250e-02,\n",
       "         1.8574e-01,  2.4432e-01,  2.5454e-01,  2.9118e-01,  4.7106e-02,\n",
       "        -2.9313e-01, -1.8963e-01, -2.5199e-01,  4.2967e-02,  2.9540e-01,\n",
       "        -1.5799e-01,  4.6764e-02, -8.0097e-02,  2.0638e-01,  9.6993e-02,\n",
       "         1.5285e-01,  1.2662e-01, -2.8163e-01, -1.0327e-01,  1.9596e-02,\n",
       "        -2.8306e-01,  2.1835e-01,  9.7796e-02, -3.9643e-01, -5.4720e-02,\n",
       "        -2.6545e-02,  7.1371e-02, -4.7886e-02,  3.1641e-01, -1.2958e-01,\n",
       "        -2.4607e-01, -9.1115e-02,  1.0304e-01, -7.7099e-02, -1.2601e-01,\n",
       "        -5.3634e-02, -1.3854e-01,  6.8302e-02, -3.0741e-02, -6.4592e-02,\n",
       "         3.6420e-02, -1.6332e-01,  2.4784e-01,  8.3879e-02,  7.4927e-02,\n",
       "         1.2919e-02,  6.1526e-02,  4.5856e-01, -1.2458e-01,  3.3233e-01,\n",
       "         6.8998e-02,  5.4167e-01, -6.8654e-02,  1.5643e-01,  2.9762e-01,\n",
       "        -1.4412e-01, -1.8248e-01,  9.3979e-02,  1.3372e-01, -9.4517e-02,\n",
       "        -2.9245e-02,  1.3328e-02,  2.3132e-01, -2.0716e-01,  2.0520e-01,\n",
       "         1.7620e-01,  7.0062e-02, -4.3567e+00, -2.5438e-01, -4.8376e-02,\n",
       "         2.4987e-02, -3.3144e-01, -2.3568e+00, -2.3523e-02,  1.2166e-01,\n",
       "         4.5342e-01, -1.8557e-01, -6.9544e-03, -6.4384e-04, -3.2180e-02,\n",
       "         2.1698e-01, -3.8292e-02, -7.8436e-03,  2.0259e-01,  7.3369e-02,\n",
       "        -7.7610e-02, -4.5818e-02,  1.0051e+00, -4.9421e-02, -2.4090e-02,\n",
       "        -3.9835e-02, -8.1303e-02,  2.3484e-01, -2.6802e-03,  7.5456e-02,\n",
       "         7.5517e-02, -6.1088e-02, -3.4001e-01, -2.0050e-03,  8.5260e-02,\n",
       "        -4.2910e-01, -2.0455e-01,  1.7620e-01, -1.2738e-01, -1.2924e-01,\n",
       "        -2.8166e-02,  1.3004e-01,  1.8133e-01, -1.2999e-01,  3.8173e-01,\n",
       "        -1.2666e-01, -6.9628e-02, -2.4997e-02, -7.4728e-02,  1.5212e-01,\n",
       "         1.6848e-02,  2.7540e-01,  3.7369e-02, -5.3197e-02,  2.1569e-02,\n",
       "         1.9244e-01, -5.3737e-01, -1.7205e-01, -3.4598e-02,  8.8368e-02,\n",
       "        -6.5704e-02, -2.4206e-02, -6.6095e-02,  1.0376e-01, -3.7068e-04,\n",
       "         1.3687e-01,  6.0686e-02,  3.8469e-02, -9.6439e-02, -3.5641e-02,\n",
       "         3.1892e-01,  1.2409e-01,  8.7088e-02, -2.5490e-01,  1.3977e-01,\n",
       "         1.1400e-01,  2.3985e-01, -5.8110e-02, -5.0688e-02, -4.4113e-02,\n",
       "        -3.9745e-01,  1.9903e-01,  3.1423e-01, -4.1974e-01,  1.5680e-01,\n",
       "        -3.3693e-02,  4.0142e-03, -8.1455e-03,  1.2431e-01, -9.1182e-02,\n",
       "        -1.1329e-01,  8.4254e-02,  2.8851e-01, -1.2806e-01, -2.3058e-01,\n",
       "         5.0058e-02,  1.6244e-01, -1.8529e-01,  3.6318e-01, -1.2112e-01,\n",
       "         1.2129e-01,  2.1088e-01, -1.6574e-02,  2.5710e-01, -1.0804e-02,\n",
       "         1.9361e-02,  2.4020e-02,  2.0548e-02, -1.2739e-01,  1.7279e-01,\n",
       "        -7.3110e-02,  1.2580e-01, -3.8492e-01,  1.6343e-01,  3.1005e-01,\n",
       "         3.2486e-02,  1.3618e-01, -7.0729e-02,  3.5091e-01, -1.1529e-01,\n",
       "        -4.1307e-02, -3.2490e-01,  1.4491e-02, -1.8125e-01,  4.9611e-03,\n",
       "         2.0495e-01,  6.8676e-02,  1.7933e-01, -1.9157e-01,  1.2187e-01,\n",
       "        -1.2256e-02, -2.5150e-01, -6.1503e-02, -1.9003e-01,  2.0790e-03,\n",
       "        -6.8694e-02,  2.2160e-01, -4.8764e-04,  1.4504e-01,  1.7988e-01,\n",
       "         5.9146e-02, -3.3442e-01, -1.6073e-01, -1.0077e-01,  9.4024e-02,\n",
       "         4.4993e-02, -3.3203e-02,  9.7097e-02, -3.0005e-01,  8.2114e-02,\n",
       "         2.9029e-01,  4.8507e-01, -3.9462e-02, -3.7702e-01, -8.5803e-02,\n",
       "         1.7896e-02,  9.7346e-02, -4.1817e-02,  1.6815e-01, -2.1356e-02,\n",
       "        -6.2688e-01,  2.8076e-01,  1.9221e-01,  9.8607e-02,  1.0979e-01,\n",
       "        -6.5303e-01,  1.0111e-01,  1.6413e-01,  1.0333e-01,  8.6067e-02,\n",
       "         5.4497e-02, -5.3538e-02,  1.1877e+00,  6.7534e-02,  1.7073e-01,\n",
       "        -6.3478e-02, -1.7339e-01,  3.4474e-02, -7.8550e-02, -3.2847e-01,\n",
       "        -2.6353e-01, -4.3065e-02,  5.5412e-02, -1.2449e-01, -1.4764e-01,\n",
       "         5.1958e-04,  9.0941e-02, -1.5660e-01, -3.1232e-01, -4.2811e-01,\n",
       "         2.4784e-01,  1.0369e-01, -6.5542e-02,  1.2535e-01,  2.2949e-02,\n",
       "        -2.5382e-01, -2.7652e-01,  1.1957e-01,  2.9252e-01,  6.6775e-02,\n",
       "         1.1647e-01,  3.1854e-01, -4.0797e-02,  2.3440e-01, -2.1806e-01,\n",
       "        -1.4424e-01,  6.9084e-02, -5.8110e-02, -8.1225e-02,  5.9624e-02,\n",
       "        -6.4126e-02, -5.9359e-01, -1.8681e-01, -6.8161e-02, -5.3035e-02,\n",
       "         1.7577e-01,  3.5808e-02,  1.8735e-01, -2.0615e-01,  3.2068e-02,\n",
       "         2.4920e-01,  6.6268e-02, -1.9527e-01,  1.9819e-01,  7.8855e-02,\n",
       "         1.2589e-01,  2.1094e-01, -2.7816e-01, -1.1749e-01,  7.1363e-02,\n",
       "         1.9217e-01,  2.4679e-01,  8.9167e-02, -2.5812e-01,  2.5816e-01,\n",
       "        -8.2562e-02, -1.1130e-05, -3.0573e-01,  3.0771e-01,  2.8462e-01,\n",
       "         4.6659e-03, -3.8497e-01, -1.0412e-01,  1.4815e-01,  1.9202e-01,\n",
       "        -1.3710e-01,  4.9604e-02,  2.5452e-01,  3.1960e-01, -2.0300e-01,\n",
       "         1.2671e-01,  2.0009e-02, -2.2593e-01,  2.5788e-01, -1.4856e-02,\n",
       "        -1.0328e+00, -2.3528e-01, -6.6942e-02,  3.9188e-01,  2.7932e-01,\n",
       "        -3.7982e-02,  3.6061e-02, -5.2175e-02,  1.7253e-01, -7.1338e-02,\n",
       "         8.7454e-02, -1.7088e-01, -1.8215e-01, -3.3924e-02, -1.0256e-01,\n",
       "         6.6021e-02,  1.3753e-01,  4.8533e-02,  1.7420e-01, -4.9254e-02,\n",
       "         2.0117e-01,  3.8660e-02,  2.4013e-01,  8.8994e-02, -1.1923e-01,\n",
       "         2.3052e-01, -2.2576e-01, -6.7364e-02, -8.4478e-02, -5.8123e-02,\n",
       "         3.5903e-02,  3.0803e-01,  3.0156e-01, -1.4505e-01,  2.3870e-01,\n",
       "         3.5255e-02,  2.4909e-01, -3.6132e-02,  1.7472e-01,  1.2282e-01,\n",
       "         8.1796e-02,  2.8296e-01,  2.3732e-01,  2.9761e-01,  9.7388e-02,\n",
       "        -1.7701e-01, -8.4918e-02,  1.1906e-01,  6.3599e-02,  2.1345e-01,\n",
       "        -1.3294e-01,  4.9109e-01, -8.3419e-02, -1.6529e-01, -2.6532e-01,\n",
       "         1.3054e-01, -3.5922e-01,  2.1678e-01, -2.7171e-01,  1.5602e-01,\n",
       "        -7.8056e-02, -2.3025e-01, -7.8927e-02,  6.1789e-02,  1.6603e-01,\n",
       "        -1.3557e-02,  1.5660e-01,  2.7801e-01, -1.0780e-01, -2.9144e-02,\n",
       "        -7.1736e-02,  4.1473e-03, -1.7400e-01,  3.6215e-01,  9.7373e-02,\n",
       "         3.6069e-01,  1.4342e-01,  8.7028e-02,  2.5010e-02, -3.3940e-01,\n",
       "         8.4403e-02,  1.7873e-01,  1.3461e-02, -1.6885e-01, -1.7230e-01,\n",
       "        -1.5217e-02,  1.9642e-01, -8.7868e-02, -2.8561e-01,  2.7861e-02,\n",
       "         1.4879e-01,  2.1931e-01,  1.4631e-01,  4.6028e-02,  5.6605e-01,\n",
       "         2.3768e-01,  5.2974e-01,  5.3234e-02, -2.6955e-01, -9.7716e-02,\n",
       "         2.8419e-01,  2.7445e-01,  3.4205e-01,  7.1666e-02, -4.5576e-02,\n",
       "        -1.4412e-01,  1.4791e-01, -5.5821e-02, -3.5106e-01, -3.5728e-02,\n",
       "        -2.6670e-01,  6.4644e-02, -1.1766e-01,  2.7318e-01,  4.5805e-01,\n",
       "         1.2551e-02,  2.3530e-01, -1.4320e-01,  1.9139e-01, -8.4488e-02,\n",
       "         1.7345e-01, -3.3824e-01,  3.9679e-01, -3.6042e+00,  2.6625e-01,\n",
       "        -7.7495e-02,  1.5842e-01,  6.5215e-02,  1.4782e-01,  9.7921e-02,\n",
       "        -1.9260e-01,  4.0207e-01, -4.7894e-01,  8.6453e-02, -8.6986e-02,\n",
       "         1.5839e-02,  6.8966e-02, -1.1194e-01,  3.4659e-02,  8.4057e-02,\n",
       "         3.3106e-03, -1.3519e-01,  1.6224e-01, -1.6465e-01, -1.2866e-01,\n",
       "         2.3671e-01,  3.2081e-01, -1.4671e-01,  3.3054e-01,  1.4982e-01,\n",
       "        -7.4960e-03, -1.8993e-01, -9.8880e-02, -5.4324e-02,  1.9397e-01,\n",
       "         2.7146e-01, -4.0923e-01,  2.9520e-01,  1.8604e-01, -1.5832e-01,\n",
       "        -1.6154e-01, -3.5280e-01, -5.2562e-02, -2.5422e-02,  1.7272e-01,\n",
       "         6.1715e-02, -6.4190e-01,  1.6685e-02,  2.2148e-01,  2.4532e-01,\n",
       "         2.3701e-02,  1.8697e-01, -1.2074e-01, -1.2671e-01,  5.0196e-02,\n",
       "         6.7698e-02, -1.2766e-01, -7.7397e-02,  5.0239e-02,  1.2101e-01,\n",
       "        -1.6446e-01, -7.2485e-02, -1.2937e-01, -1.8631e-02, -1.6012e-01,\n",
       "        -1.3036e-01,  1.3461e-01, -1.6057e-01, -2.9725e-01,  1.4073e-01,\n",
       "        -1.1538e-01, -1.7267e-01, -2.5250e-01, -1.7082e-01,  6.5832e-02,\n",
       "         1.3484e-01, -1.4126e-01,  2.4372e-01, -7.6884e-02, -2.2166e-01,\n",
       "        -8.2886e-03,  6.4718e-03,  1.8528e-01,  2.4171e-01, -2.3589e-01,\n",
       "        -2.1638e-02,  1.7351e-01, -4.4954e-02, -4.0453e-03,  4.0088e-02,\n",
       "         2.8900e-01, -2.3135e-01,  2.4702e-01, -1.1958e-02,  1.5234e-01,\n",
       "         1.7264e-02, -3.7429e-02, -6.6217e-02,  1.0786e-01,  1.1063e-01,\n",
       "         1.6744e-01, -6.3835e-01,  6.6903e-03,  1.5101e-01, -5.0015e-01,\n",
       "         2.2181e-02,  1.9575e-02, -3.3730e-01, -7.0082e-02,  2.7466e-01,\n",
       "        -7.7125e-02,  4.2710e-01,  1.0638e-01,  1.1128e-01, -2.4339e-01,\n",
       "        -1.0703e-01, -1.1123e-01, -3.9697e-02,  6.1332e-02,  2.9097e-01,\n",
       "         3.0467e-01,  8.0104e-02, -1.4272e-02, -2.6584e-02, -1.9677e-02,\n",
       "         3.0377e-02, -2.2273e-01,  3.3940e-01, -2.1156e-01, -7.0498e-02,\n",
       "         1.3893e-01,  3.3360e-01, -3.1318e-01,  1.3757e-01, -3.4007e-02,\n",
       "        -4.0606e-01, -2.2081e-01, -8.4412e-02,  1.0916e+01,  1.4774e-01,\n",
       "         6.7685e-02,  1.4407e-01, -6.1927e-03,  2.0313e-02, -5.4902e-03,\n",
       "        -1.9441e-01,  2.1274e-01,  1.1476e-01,  7.5072e-04, -1.7262e-01,\n",
       "        -2.2017e-01,  3.4906e-02,  1.9373e-01,  9.7054e-02,  1.3715e-01,\n",
       "         4.1155e-01,  6.6535e-02,  6.5653e-02,  2.3882e-01,  3.0348e-02,\n",
       "        -2.9760e-01, -2.6605e-01,  2.4166e-02,  1.1041e-01,  4.5244e-02,\n",
       "        -1.9347e-01, -2.8693e-01,  8.8830e-02, -9.1318e-02,  1.5866e-01,\n",
       "        -6.3153e-02,  2.6066e-01,  8.5028e-02,  2.1086e-02, -4.3173e-01,\n",
       "        -1.9664e-01,  1.4895e-01, -3.0689e-03,  1.8683e-01, -4.4704e-02,\n",
       "        -6.2430e-02,  3.0073e-01, -7.9790e-02, -7.4392e-02, -9.5084e-02,\n",
       "         1.6347e-01,  3.6079e-02, -2.3387e-01,  5.8242e-02,  2.5627e-01,\n",
       "         2.2990e-01,  8.0629e-02,  1.8864e-01,  1.7794e-01,  2.3047e-01,\n",
       "         3.2501e-01, -1.6969e-01,  1.6728e-01, -2.3926e-01,  1.4501e-02,\n",
       "         8.0097e-03,  1.2529e-01,  3.9714e-01,  7.3738e-03,  7.6656e-02,\n",
       "         4.0899e-01,  4.9764e-02, -7.1896e-02, -1.7154e-01, -7.9515e-02,\n",
       "        -4.8641e-02,  2.6657e-01, -1.4039e-01,  1.1023e-01, -5.9699e-02,\n",
       "        -2.0848e-01,  4.3334e-02, -7.0110e-01,  2.7376e-01,  1.4685e-01,\n",
       "        -1.8592e-01, -7.0154e-02, -7.7791e-02,  4.2772e-02,  1.1944e-01,\n",
       "        -4.4813e-02, -1.5206e-01,  9.5230e-02, -2.8025e-02,  1.7968e-01,\n",
       "        -3.7414e-02,  2.5817e-01, -4.9797e-02, -1.7559e-01, -1.8158e-01,\n",
       "         7.8075e-04, -1.5381e-01,  3.5257e-01, -2.7358e-01, -1.5675e-01,\n",
       "         1.9750e-01,  1.5934e-01, -2.2482e-01,  2.0093e-01, -3.9200e-01,\n",
       "         4.5001e-02, -9.3651e-02,  6.8662e-02,  2.4401e-01,  6.0815e-02,\n",
       "         2.2360e-02,  1.0611e-01, -2.1079e-01,  2.0549e-01, -1.1132e-01,\n",
       "         5.7199e-02,  1.3799e-01,  2.3461e-01, -1.9130e-01, -3.9758e-02,\n",
       "         2.2427e-02, -7.7258e-02,  4.5518e-02, -1.8518e-01,  2.7092e-01,\n",
       "        -7.1506e-02,  9.2513e-02, -7.4220e-02, -1.8658e-01, -5.4472e-02,\n",
       "         5.2939e-02, -1.2633e-01, -1.0607e-01,  3.7747e-02,  1.6098e-01,\n",
       "         3.6562e-02,  4.0984e-02,  1.1743e-01, -9.7527e-02,  2.1477e-01,\n",
       "        -8.3945e-02, -7.7206e-01,  3.2399e-02, -3.3064e-01, -6.5697e-03,\n",
       "         7.9503e-02, -1.3257e-01, -1.5841e-02,  3.3419e-01,  1.8783e-01,\n",
       "        -3.2496e-01,  1.3119e-01,  6.6200e-03,  1.7647e-01, -1.7002e-01,\n",
       "         1.0558e-01,  9.6697e-02, -2.0191e-01,  2.5888e-01, -2.5460e-01,\n",
       "         1.4620e-01,  3.9548e-02,  1.6326e-01,  3.6664e-01,  6.7034e-02,\n",
       "        -6.6205e-02,  3.2539e-01,  8.7532e-02,  6.1805e-02,  1.9844e-01,\n",
       "        -2.4654e-01, -1.2702e-01, -1.5617e-01,  5.3066e-03,  5.6255e-01,\n",
       "         2.5164e-01, -4.9737e-02, -2.0023e-02])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "36d7bccc",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'last_hidden_layer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5819/3700560832.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'last_hidden_layer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/biu_task2/lib/python3.7/site-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m             \u001b[0minner_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 271\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'last_hidden_layer'"
     ]
    }
   ],
   "source": [
    "model_output['last_hidden_layer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "a0bd696b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "<mask> : 50264\n",
      " am : 524\n",
      " so : 98\n",
      "<mask> : 50264\n",
      "</s> : 2\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(\"<mask> am so <mask>\", return_tensors=\"pt\")\n",
    "inputs['output_hidden_states'] = True\n",
    "for code in inputs['input_ids'][0]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model_output = RobertaLM_model(**inputs)\n",
    "    logits = model_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "bc29a4f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 50265])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "45219845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 625]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"sad\"])['input_ids'][0][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ac554b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5819/2804495166.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sad\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tokenizer([\"sad\"])['input_ids']["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "b276ccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[29, 625]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"sad\"])['input_ids'][0][1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "931a5e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "efeb51f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3564, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "all_embeddings = RobertaLM_model.roberta.embeddings.word_embeddings.weight\n",
    "\n",
    "# I am very \"happy\"\n",
    "embeddings1 = torch.mean(all_embeddings[tokenizer([\"good\"])['input_ids'][0][1:-1]], dim=0)\n",
    "embeddings2 = torch.mean(all_embeddings[[tokenizer([\"bad\"])['input_ids'][0][1:-1]]], dim=0)\n",
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "be6f11ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "77cbbf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1211, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_similarity = torch.nn.CosineSimilarity(dim=0)\n",
    "cos_similarity(embeddings1, embeddings2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "8448653e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-3.9688e-02,  6.4377e-02,  3.9093e-02, -8.1921e-02, -1.5125e-01,\n",
       "         4.9866e-02,  1.8823e-01, -8.3008e-02, -1.1902e-01,  2.3880e-03,\n",
       "        -1.8250e-02,  1.2848e-02, -9.2865e-02,  8.4457e-02, -2.8625e-02,\n",
       "         1.0071e-03, -5.4413e-02, -6.9824e-02, -9.2026e-02, -5.6358e-02,\n",
       "         1.3062e-02, -1.6934e-01, -8.8196e-03, -6.3446e-02,  3.0323e-02,\n",
       "        -2.1118e-01, -5.0087e-02, -1.9440e-01, -1.9006e-01, -1.8185e-01,\n",
       "         2.6913e-02, -8.2920e-02,  3.0212e-03, -2.0068e-01, -2.3346e-03,\n",
       "        -2.9348e-02, -7.5556e-02, -5.6000e-02, -1.0611e-01, -6.6086e-02,\n",
       "         2.6611e-02, -2.3730e-01, -9.0897e-02, -1.3884e-01,  7.2021e-03,\n",
       "        -1.0330e-02, -3.4409e-02,  8.4778e-02,  7.9041e-02,  4.4128e-02,\n",
       "         7.7179e-02,  7.9803e-02, -1.0681e-02, -7.7698e-02,  5.9509e-03,\n",
       "        -9.3523e-02, -1.8066e-02, -5.6702e-02,  2.4956e-02,  4.8828e-04,\n",
       "         2.8206e-02, -4.0741e-02, -2.8290e-02, -2.0874e-02,  8.6433e-02,\n",
       "        -3.7292e-02, -1.5564e-02, -9.2773e-03,  6.4423e-02, -9.4513e-02,\n",
       "        -1.6840e-01, -1.1740e-01, -3.4977e-02, -9.0302e-02, -2.6093e-02,\n",
       "         2.0813e-02,  2.2949e-01, -3.7659e-02, -7.5645e-03,  1.8433e-02,\n",
       "        -1.2497e-01, -4.9072e-02,  4.5517e-02,  1.0555e-01,  9.7656e-03,\n",
       "        -1.4923e-01, -2.1352e-02, -1.4270e-01,  4.7774e-02, -3.1708e-02,\n",
       "        -6.2561e-04, -9.2621e-02, -4.4907e-02, -9.8587e-02, -5.5923e-02,\n",
       "        -1.2018e-01,  1.9684e-02, -1.1563e-01, -4.5120e-02, -7.9010e-02,\n",
       "         4.0947e-02,  3.0701e-02,  4.1122e-02,  1.1166e-01, -4.4991e-02,\n",
       "        -4.0009e-02,  3.1403e-02,  7.1014e-02, -1.0883e-01,  1.3718e-02,\n",
       "        -5.2078e-02, -1.0211e-01, -8.8547e-02,  5.6183e-02, -1.0731e-02,\n",
       "        -4.0894e-03, -6.3293e-02, -8.6685e-02,  5.9372e-02, -1.5585e-01,\n",
       "         5.8167e-02, -5.4688e-02,  3.8452e-03,  2.1515e-02, -1.8799e-02,\n",
       "         7.7671e-02,  1.8677e-02, -1.5625e-01, -5.2383e-02, -8.3981e-03,\n",
       "        -3.3348e-02,  3.8496e-02, -9.0227e-03, -1.0231e-02, -1.7609e-02,\n",
       "         5.5885e-02,  1.5076e-02,  1.1589e-02, -2.6800e-02,  6.7139e-02,\n",
       "        -1.2439e-01, -3.8773e-02, -1.4199e-01, -6.5125e-02, -1.7004e-01,\n",
       "        -4.9282e-02, -8.0963e-02, -1.0471e-01, -2.6230e-02, -6.0371e-02,\n",
       "         1.1154e-02, -2.0325e-02,  6.3124e-03, -7.2105e-02, -1.6174e-03,\n",
       "        -4.2198e-02, -4.0794e-02, -5.0131e-02, -8.3008e-02,  1.3123e-02,\n",
       "        -4.6440e-02, -2.4982e-01,  2.5421e-02,  6.8771e-02, -9.0881e-02,\n",
       "         6.3629e-02,  1.4099e-02,  1.5833e-01,  7.4234e-03,  2.7893e-02,\n",
       "         5.3650e-02, -1.0669e-01, -6.0349e-02, -1.2115e-01, -6.4636e-02,\n",
       "        -1.0912e-01,  3.5501e-02, -1.0056e-01, -3.2768e-02, -1.1713e-01,\n",
       "        -1.3791e-01, -9.6817e-02,  4.1138e-02, -3.4119e-02, -5.4413e-02,\n",
       "         5.2338e-02, -1.2378e-01,  6.8100e-02,  1.1330e-01, -8.9417e-03,\n",
       "        -6.9183e-02,  2.5391e-02,  8.3344e-02,  1.5839e-01, -1.0742e-01,\n",
       "         8.3466e-02, -7.8873e-02,  3.3409e-02, -4.1168e-02, -1.9437e-01,\n",
       "         4.9494e-02, -8.6395e-02,  1.0486e-01, -1.4410e-01, -1.0223e-02,\n",
       "        -2.8637e-02, -1.4038e-03, -1.1143e-01,  1.9676e-02, -1.0117e-01,\n",
       "        -1.6983e-02, -4.5692e-02,  1.1737e-01,  1.9012e-02, -4.4144e-02,\n",
       "        -1.1169e-01,  1.1627e-01, -9.1232e-02, -1.5179e-01, -2.4338e-03,\n",
       "        -2.2858e-01, -6.7261e-02, -1.1763e-01, -8.9142e-02, -4.8370e-02,\n",
       "        -2.7771e-03, -8.7158e-02, -9.0454e-02, -3.8910e-03,  1.6127e-01,\n",
       "         4.0726e-02,  2.0966e-02, -1.8207e-01,  3.0808e-02,  5.4764e-02,\n",
       "         1.0960e-01,  1.5158e-01,  1.3896e-01,  6.2180e-02,  9.5684e-02,\n",
       "         1.1691e-01,  1.7908e-01,  2.1606e-02,  1.6891e-02,  1.7174e-02,\n",
       "         4.3045e-02, -8.0261e-02,  5.1891e-02, -1.0797e-01, -1.1490e-01,\n",
       "        -4.0894e-02, -5.7373e-03,  2.2495e-02, -1.9997e-02, -5.4871e-02,\n",
       "        -9.5204e-02, -3.8387e-02, -2.5795e-02,  5.3596e-02, -1.9220e-01,\n",
       "         3.6995e-02,  1.5585e-01,  4.8981e-02,  1.1429e-01, -1.3031e-01,\n",
       "        -2.9499e-02, -8.9233e-02, -2.6848e-02, -7.2739e-02,  7.4310e-03,\n",
       "        -5.9967e-02,  2.7920e-02, -1.1697e-01,  5.4901e-02,  3.6255e-02,\n",
       "        -1.4734e-01, -8.5510e-02, -1.4666e-01,  4.4838e-02,  8.5495e-02,\n",
       "        -4.6261e-02,  2.4933e-02, -1.2198e-01,  2.1906e-01,  1.1108e-02,\n",
       "        -1.6885e-01,  3.1036e-02, -1.1261e-02, -2.2453e-02, -1.3196e-01,\n",
       "         7.3151e-02, -8.0475e-02,  2.7969e-02, -7.8213e-02, -3.2608e-02,\n",
       "        -6.1676e-02,  2.7763e-02, -2.6619e-02,  2.0164e-01, -2.7451e-02,\n",
       "         3.2593e-02,  4.0897e-02,  1.8304e-01, -8.3618e-03,  6.3671e-02,\n",
       "        -5.2246e-02, -4.5731e-02,  1.3908e-01, -1.5625e-02,  2.0950e-02,\n",
       "        -1.1227e-01, -8.4991e-02, -8.9912e-02,  7.1655e-02, -5.6843e-02,\n",
       "         1.4038e-03, -4.1611e-02,  5.8990e-02, -1.0623e-01, -2.1454e-02,\n",
       "         1.0463e-01, -9.0851e-02, -1.7334e-02, -4.1428e-02,  1.0254e-02,\n",
       "        -1.1368e-01, -3.9406e-02,  1.5515e-01, -6.3889e-02,  3.6560e-02,\n",
       "         7.8003e-02, -7.2937e-03, -4.3293e-02,  2.8872e-02, -1.0858e-01,\n",
       "         3.4760e-02, -2.6875e-02, -2.2263e-02, -9.5703e-02, -4.7211e-02,\n",
       "        -1.7556e-02, -1.1253e-01,  1.2204e-01, -9.5428e-02,  6.6544e-02,\n",
       "        -6.8306e-02, -1.1127e-01,  1.6850e-02, -9.9461e-02, -7.4326e-02,\n",
       "         6.6376e-04,  1.5714e-01, -6.3339e-02, -8.0811e-02,  4.1817e-02,\n",
       "         6.8490e-02, -1.5720e-01,  2.1890e-02, -6.9290e-02, -2.4581e-02,\n",
       "         1.2695e-02, -1.7406e-02, -3.0090e-02, -1.8424e-02,  3.1147e-03,\n",
       "        -6.5842e-02, -1.9553e-01, -4.9255e-02,  1.8570e-02,  1.3191e-02,\n",
       "        -1.2192e-01, -1.3309e-01,  1.1244e-01, -1.4853e-01,  3.0457e-02,\n",
       "        -7.6233e-02, -5.4626e-03, -5.3164e-02, -7.2725e-02, -4.3182e-02,\n",
       "        -2.6962e-02, -1.6754e-01, -1.2366e-01, -6.7413e-02, -1.0434e-01,\n",
       "        -2.2150e-01, -2.4200e-02, -3.8177e-02, -2.7527e-02, -8.0032e-02,\n",
       "         6.9542e-02,  7.5684e-03, -1.3403e-01, -3.1891e-03, -3.7239e-02,\n",
       "        -1.4972e-01, -1.0141e-01,  5.0064e-02, -9.1232e-02,  3.3356e-02,\n",
       "         6.8665e-02,  1.7871e-01, -7.4600e-02, -9.9380e-02, -1.5759e-01,\n",
       "         5.5145e-02,  5.1796e-02, -3.4821e-02, -8.0353e-02,  1.2695e-02,\n",
       "        -1.0367e-01,  3.1372e-02,  1.4496e-03,  1.0447e-01,  1.4059e-01,\n",
       "        -9.6443e-02, -1.2354e-01,  1.1366e-01, -1.3565e-02, -8.2397e-03,\n",
       "        -2.8336e-02, -1.3461e-01,  1.9531e-03,  1.2250e-01,  6.6406e-02,\n",
       "        -1.5610e-01,  3.6102e-02,  9.2701e-02, -8.8211e-02,  4.0461e-02,\n",
       "        -6.6971e-02, -1.9165e-02, -1.3013e-01, -3.6621e-04, -8.7433e-02,\n",
       "        -3.7384e-03,  4.4342e-02,  1.8347e-01,  1.0376e-02,  6.7377e-02,\n",
       "        -2.2562e-01, -2.8033e-01, -6.7757e-02,  6.4087e-04, -5.2261e-02,\n",
       "        -2.3071e-02, -1.5692e-01, -3.3306e-02, -2.8496e-03, -9.5749e-02,\n",
       "        -8.1196e-03, -1.7654e-02, -5.2448e-02, -1.8802e-01, -1.2308e-01,\n",
       "        -3.2063e-02, -1.8353e-01, -8.8448e-02, -1.1917e-02,  4.8660e-02,\n",
       "         3.1097e-02,  3.0903e-02,  5.6335e-02, -6.5079e-03,  6.4869e-03,\n",
       "        -5.4115e-02, -2.3300e-02,  3.0298e-01, -1.7639e-02,  8.8678e-02,\n",
       "        -2.9175e-02, -1.4015e-01, -5.2490e-03, -9.1644e-02, -2.4567e-02,\n",
       "         1.0497e-01, -4.8492e-02, -1.9684e-02, -6.5598e-02, -6.8420e-02,\n",
       "         1.7227e-02,  3.3546e-02, -9.1248e-02, -6.7749e-03, -5.1971e-02,\n",
       "         7.9613e-02, -3.9520e-02,  7.3257e-02,  3.5736e-02, -5.6305e-02,\n",
       "         1.0880e-02, -1.5936e-01, -4.5349e-02, -4.5609e-02, -3.2043e-02,\n",
       "        -4.3861e-02, -5.2879e-02, -1.3794e-02,  2.8839e-02,  1.6406e-01,\n",
       "        -2.1851e-02, -2.9312e-02,  1.5350e-02, -4.4350e-02,  3.1731e-02,\n",
       "        -5.2399e-02, -1.0904e-01,  9.6588e-02,  2.0782e-02, -9.8572e-03,\n",
       "        -1.0355e-01,  4.8428e-02, -1.5698e-01, -1.6760e-01,  5.2399e-02,\n",
       "        -2.0782e-02, -6.4194e-02, -1.2109e-01, -1.3568e-01, -1.2411e-01,\n",
       "         7.7483e-02,  3.9291e-02, -2.4597e-02, -8.8181e-02, -7.8400e-02,\n",
       "         5.6808e-02, -4.0073e-02, -9.2111e-02, -6.0883e-02, -4.3938e-02,\n",
       "        -1.4664e-02, -8.6975e-02, -3.9454e-02, -1.3550e-02, -4.8111e-02,\n",
       "        -4.4067e-02, -7.3532e-02,  1.3306e-02, -4.0146e-02, -5.5656e-02,\n",
       "        -2.9053e-02, -9.5978e-02,  1.5128e-01, -8.5142e-02, -6.2134e-02,\n",
       "         4.6219e-02, -3.6156e-02, -1.0559e-01,  7.6599e-03, -1.6275e-01,\n",
       "        -1.6130e-01,  5.4611e-02, -9.8450e-02,  5.0285e-02,  6.7909e-02,\n",
       "        -1.9733e-01, -8.2626e-03, -1.5503e-02,  1.9379e-01,  5.4054e-03,\n",
       "        -4.6860e-02, -1.0342e-01, -1.1362e-01,  9.2834e-02, -6.1707e-02,\n",
       "        -5.5206e-02, -9.6863e-02, -2.1667e-03,  6.2469e-02,  4.6753e-02,\n",
       "         7.5607e-02, -8.5400e-02, -7.5073e-02, -5.7270e-02, -2.0958e-02,\n",
       "        -4.4258e-02, -5.8880e-02,  6.5437e-02,  3.9736e-02, -1.4517e-01,\n",
       "        -8.6502e-02, -7.3242e-03,  8.6060e-03, -1.1780e-01, -3.1876e-02,\n",
       "        -1.3916e-01, -8.2184e-02,  1.8883e-02,  3.4073e-02,  9.3994e-02,\n",
       "        -7.3531e-02,  8.0688e-02,  6.3171e-03,  3.4454e-02, -3.6774e-03,\n",
       "        -9.7412e-02,  4.8779e-02, -6.4529e-02,  4.6734e-02,  9.1248e-02,\n",
       "        -4.8740e-02,  8.0002e-02, -1.1272e-01, -7.1213e-02,  2.1484e-02,\n",
       "        -1.3489e-02,  5.9769e-02,  4.1492e-02,  6.1035e-05,  1.3779e-01,\n",
       "         2.6306e-02, -1.0907e-01, -3.9520e-02,  9.4679e-02, -9.5673e-02,\n",
       "        -1.5762e-02,  7.5134e-02,  5.1048e-02, -1.9362e-01, -4.7905e-02,\n",
       "         8.4663e-02, -6.0486e-02, -2.8885e-02, -1.8039e-01, -1.3080e-01,\n",
       "        -2.2049e-03, -3.9246e-02,  6.2759e-02,  1.0672e-01, -1.1316e-01,\n",
       "        -3.2961e-02, -1.4191e-02, -8.2626e-02,  5.4558e-02, -4.3145e-02,\n",
       "         4.9557e-02, -2.5185e-02,  4.6997e-02,  1.3428e-03, -1.8103e-01,\n",
       "        -4.7256e-02,  5.5313e-02, -8.8488e-02,  4.6356e-02, -6.0730e-02,\n",
       "         6.6040e-02, -4.3442e-02,  4.6112e-02, -7.7515e-03,  2.8091e-02,\n",
       "        -7.9132e-02, -5.6004e-02,  1.1308e-01, -8.9378e-02, -1.6769e-01,\n",
       "         5.0049e-03,  2.2797e-01,  7.3410e-02, -1.2241e-01,  2.3628e-02,\n",
       "        -8.9783e-02,  8.7677e-02,  6.3892e-02, -1.2426e-02,  2.6077e-02,\n",
       "        -7.8308e-02,  3.4088e-02, -2.0111e-01, -7.2510e-02,  2.2240e-02,\n",
       "         6.8617e-02, -9.3666e-02,  6.7261e-02, -1.0654e-01, -5.0110e-02,\n",
       "        -3.7376e-02, -2.8610e-02, -2.0679e-01, -7.8217e-02, -4.8386e-02,\n",
       "        -9.7839e-02, -5.3436e-02, -5.6183e-02, -3.6301e-02, -9.8171e-03,\n",
       "         2.5558e-03, -5.3467e-02, -8.4183e-02,  5.6259e-02,  3.6209e-02,\n",
       "         5.5237e-03, -1.8965e-02, -4.6555e-02, -6.0677e-02, -1.0864e-02,\n",
       "        -8.0887e-02,  8.1024e-02, -8.6060e-02, -8.4076e-03, -1.5564e-02,\n",
       "         5.3772e-02,  1.1661e-01, -7.2327e-02, -1.2387e-01, -1.1319e-01,\n",
       "         3.1799e-02, -1.1475e-02, -1.2526e-01,  1.9989e-02,  1.0617e-01,\n",
       "        -8.4961e-02, -7.5216e-02,  8.0956e-02,  2.6245e-03, -1.6785e-02,\n",
       "        -1.9189e-01,  3.1265e-02, -4.6501e-02, -1.0545e-01, -7.7026e-02,\n",
       "         4.8828e-04,  5.2407e-02,  7.7236e-02, -1.2128e-01, -6.9672e-02,\n",
       "         1.2906e-01, -1.2848e-02,  7.0374e-02, -7.0301e-02, -6.4682e-02,\n",
       "        -6.1340e-02, -9.5398e-02, -5.4047e-02,  6.6681e-03, -4.6524e-02,\n",
       "        -1.7230e-01, -1.2531e-02, -4.2786e-02, -1.0535e-01, -3.8616e-02,\n",
       "        -6.9052e-02, -2.3293e-02, -6.8741e-02, -1.1018e-01, -3.0457e-02,\n",
       "         3.4805e-02, -4.3779e-02, -1.0861e-01, -3.6171e-02, -1.0855e-01,\n",
       "        -1.0565e-01, -4.2770e-02,  1.3672e-02,  5.0354e-04, -5.4688e-02,\n",
       "        -2.0117e-01,  5.5386e-02, -7.4219e-02, -4.5837e-02, -8.4595e-02,\n",
       "        -3.9143e-02, -1.3254e-01, -3.9188e-02, -8.2007e-02, -3.3356e-02,\n",
       "        -4.2419e-03, -1.0242e-01, -4.0283e-03], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(embeddings2,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b8167854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 27333, 2], [0, 29, 625, 2]], 'attention_mask': [[1, 1, 1], [1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"happy\", \"sad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7c6775a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 27333, 2]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"happy\", \"sad\"])['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1e42d14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "m : 119\n",
      "iser : 5999\n",
      "able : 868\n",
      "</s> : 2\n"
     ]
    }
   ],
   "source": [
    "for code in tokenizer([\"happy\", \"miserable\"])['input_ids'][1]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb09b3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "ad15640b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "I : 100\n",
      " am : 524\n",
      " so : 98\n",
      "<mask> : 50264\n",
      "</s> : 2\n"
     ]
    }
   ],
   "source": [
    "Roberta_model = RobertaModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer([\"I am very\", return_tensors=\"pt\")\n",
    "inputs['output_hidden_states'] = True\n",
    "for code in inputs['input_ids'][0]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    last_hidden_state = Roberta_model(**inputs)['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "75681e55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states'])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6303c253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaModel\n",
    "model2 = RobertaModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "23dcc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    model_output2 = model2(**inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9422d2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_output2['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f393dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Roberta_model = RobertaModel.from_pretrained(model_checkpoint)\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs['output_hidden_states'] = True\n",
    "for code in inputs['input_ids'][0]:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "    \n",
    "with torch.no_grad():\n",
    "    model_output = model(**inputs)\n",
    "    logits = model_output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66b1b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7960a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RobertaModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d9eb366b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model_output['hidden_states'])#[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d5c86f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a42759f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['logits', 'hidden_states'])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6718e457",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForMaskedLM(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lm_head): RobertaLMHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=768, out_features=50265, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e7b124cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> : 0\n",
      "D : 495\n",
      "il : 718\n",
      "oph : 6673\n",
      "osaurus : 44422\n",
      "</s> : 2\n"
     ]
    }
   ],
   "source": [
    "# 4) Find a sentence with n words, that is tokenized into m > n tokens by the tokenizer.\n",
    "for code in tokenizer(\"Dilophosaurus\")['input_ids']:\n",
    "    print(f\"{ tokenizer.decode(code)} : {code}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672507a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
